from flask import Blueprint, request, jsonify, send_file
from datetime import datetime
import json
import logging
import asyncio
import functools
import os
import sys
from typing import List, Dict
from dataclasses import asdict
import concurrent.futures

# æ·»åŠ çˆ¶ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models.database import get_db
from utils.git_utils import GitUtils
# WebSocketç›¸å…³å¯¼å…¥
try:
    from websocket_server import send_notification
except ImportError:
    logger.warning("WebSocket server not available")
    def send_notification(*args, **kwargs):
        pass

# åˆ†æå™¨ç›¸å…³å¯¼å…¥
try:
    from analyzers.enhanced_cursor_analyzer import EnhancedCursorAnalyzer
except ImportError:
    logger.warning("Enhanced cursor analyzer not available")
    EnhancedCursorAnalyzer = None



# å°è¯•å¯¼å…¥å¯é€‰æ¨¡å—
try:
    from indexers.codebase_indexer import CodebaseIndexer
except ImportError:
    class CodebaseIndexer:
        def __init__(self, *args, **kwargs):
            pass
        def index_codebase(self, *args, **kwargs):
            return {'error': 'CodebaseIndexer not available'}

try:
    from utils.html_reporter import HTMLReporter
except ImportError:
    class HTMLReporter:
        def __init__(self, *args, **kwargs):
            pass
        def generate_report(self, *args, **kwargs):
            return '<html><body>HTML Reporter not available</body></html>'

try:
    from utils.json_reporter import JSONReporter
except ImportError:
    class JSONReporter:
        def __init__(self, *args, **kwargs):
            pass
        def generate_report(self, *args, **kwargs):
            return {'error': 'JSON Reporter not available'}

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger('api')

api = Blueprint('api', __name__)

db = get_db()

# å…¨å±€äº‹ä»¶å¾ªç¯ï¼Œç¡®ä¿æ‰€æœ‰å¼‚æ­¥æ“ä½œåœ¨åŒä¸€ä¸ªäº‹ä»¶å¾ªç¯ä¸­è¿è¡Œ
_loop = None

def get_loop():
    """è·å–å…¨å±€äº‹ä»¶å¾ªç¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º"""
    global _loop
    if _loop is None or _loop.is_closed():
        _loop = asyncio.new_event_loop()
        asyncio.set_event_loop(_loop)
    return _loop

def run_async(func):
    """è¿è¡Œå¼‚æ­¥å‡½æ•°çš„åŒæ­¥åŒ…è£…å™¨"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        loop = get_loop()
        try:
            return loop.run_until_complete(func(*args, **kwargs))
        except Exception as e:
            logger.error(f"å¼‚æ­¥æ“ä½œå¤±è´¥: {str(e)}")
            raise
    return wrapper

# æ·»åŠ CORSé¢„æ£€è¯·æ±‚å¤„ç†
@api.route('/', methods=['OPTIONS'])
@api.route('/<path:path>', methods=['OPTIONS'])
def handle_options(path=None):
    return '', 204

# æ·»åŠ æ ¹è·¯ç”±ï¼Œé¿å…404
@api.route('/', methods=['GET'])
def index():
    return jsonify({
        'message': 'ä»£ç åˆ†æåç«¯APIæœåŠ¡',
        'status': 'running',
        'endpoints': ['/api/projects', '/api/projects/<id>']
    })

@api.route('/projects', methods=['GET'])
def get_projects():
    try:
        projects = db.fetch_all("SELECT * FROM projects ORDER BY created_at DESC")
        
        # å¤„ç†SQLiteä¸­çš„JSONå­—æ®µ
        for project in projects:
            if 'stats' in project and project['stats'] and isinstance(project['stats'], str):
                try:
                    project['stats'] = json.loads(project['stats'])
                except:
                    project['stats'] = {}
        
        return jsonify(projects)
    except Exception as e:
        logger.error(f"è·å–é¡¹ç›®åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify([]), 500

@api.route('/projects/<int:id>', methods=['GET'])
def get_project(id):
    try:
        project = db.fetch_one("SELECT * FROM projects WHERE id = %s", (id,))
        
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        # å¤„ç†SQLiteä¸­çš„JSONå­—æ®µ
        if 'stats' in project and project['stats'] and isinstance(project['stats'], str):
            try:
                project['stats'] = json.loads(project['stats'])
            except:
                project['stats'] = {}
        
        # è·å–é¡¹ç›®çš„åˆ†æç»“æœ
        analysis_results = db.fetch_all(
            "SELECT * FROM analysis_results WHERE project_id = %s ORDER BY created_at DESC", 
            (id,)
        )
        
        # å¤„ç†åˆ†æç»“æœä¸­çš„JSONå­—æ®µ
        for result in analysis_results:
            if 'result_data' in result and result['result_data'] and isinstance(result['result_data'], str):
                try:
                    result['result_data'] = json.loads(result['result_data'])
                except:
                    result['result_data'] = {}
            
            # è·å–æµ‹è¯•ç”¨ä¾‹
            test_cases = db.fetch_all(
                "SELECT * FROM test_cases WHERE analysis_id = %s",
                (result['id'],)
            )
            result['testCases'] = test_cases
        
        # å°è¯•å…‹éš†å¹¶åˆ†æä»“åº“ï¼Œè·å–æœ€æ–°ç»Ÿè®¡ä¿¡æ¯
        try:
            git_utils = GitUtils(project['git_url'], project['branch'])
            if git_utils.clone_repo():
                # è·å–ä»“åº“ç»Ÿè®¡ä¿¡æ¯
                stats = git_utils.get_repo_stats()
                if stats:
                    # æ›´æ–°é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯
                    project['stats'] = stats
                    
                    # æ›´æ–°æ•°æ®åº“ä¸­çš„ç»Ÿè®¡ä¿¡æ¯
                    stats_json = json.dumps(stats)
                    db.execute(
                        "UPDATE projects SET stats = %s WHERE id = %s",
                        (stats_json, id)
                    )
                
                # æ¸…ç†ä¸´æ—¶ç›®å½•
                git_utils.cleanup()
        except Exception as e:
            logger.error(f"è·å–ä»“åº“ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        
        return jsonify({
            'project': project,
            'analysis_results': analysis_results
        })
    except Exception as e:
        logger.error(f"è·å–é¡¹ç›®è¯¦æƒ…å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@api.route('/projects', methods=['POST'])
def create_project():
    try:
        data = request.json
        name = data.get('name')
        git_url = data.get('git_url')
        branch = data.get('branch', 'main')
        description = data.get('description', '')
        
        if not name or not git_url:
            return jsonify({'error': 'Name and git_url are required'}), 400
        
        # æ³¨é‡Šæ‰é‡å¤æ£€æŸ¥ï¼Œå…è®¸ç›¸åŒä»“åº“çš„å¤šä¸ªé…ç½®
        # try:
        #     existing_project = db.fetch_one("SELECT * FROM projects WHERE git_url = %s", (git_url,))
        #     if existing_project:
        #         return jsonify({'error': 'è¯¥ä»“åº“è·¯å¾„å·²è¢«ä½¿ç”¨'}), 409
        # except Exception as e:
        #     logger.error(f"æ£€æŸ¥ä»“åº“è·¯å¾„å¤±è´¥: {str(e)}")
        #     # ç»§ç»­æ‰§è¡Œï¼Œä¸é˜»æ­¢åˆ›å»ºé¡¹ç›®

        # éªŒè¯ä»“åº“æ˜¯å¦å¯è®¿é—®
        git_utils = GitUtils(git_url, branch)
        if not git_utils.validate_repo():
            return jsonify({'error': 'æ— æ³•è®¿é—®è¯¥Gitä»“åº“ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®'}), 400
        
        # å°è¯•å…‹éš†ä»“åº“å¹¶è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = None
        if git_utils.clone_repo():
            stats = git_utils.get_repo_stats()
            git_utils.cleanup()
        
        # åˆ›å»ºé¡¹ç›®
        stats_json = json.dumps(stats) if stats else None
        result = db.execute(
            "INSERT INTO projects (name, git_url, branch, description, created_at, stats) VALUES (%s, %s, %s, %s, %s, %s) RETURNING id",
            (name, git_url, branch, description, datetime.now(), stats_json)
        )
        
        # å¤„ç†ä¸åŒæ•°æ®åº“çš„è¿”å›å€¼æ ¼å¼
        if isinstance(result, int):
            # SQLiteè¿”å›lastrowidï¼ˆæ•´æ•°ï¼‰
            project_id = result
        elif result and 'id' in result:
            # PostgreSQLè¿”å›å­—å…¸
            project_id = result['id']
        else:
            return jsonify({'error': 'é¡¹ç›®åˆ›å»ºå¤±è´¥'}), 500
        
        return jsonify({'id': project_id, 'message': 'é¡¹ç›®åˆ›å»ºæˆåŠŸ'}), 201
    except Exception as e:
        logger.error(f"å¤„ç†åˆ›å»ºé¡¹ç›®è¯·æ±‚å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@api.route('/projects/<int:id>', methods=['PUT'])
def update_project(id):
    try:
        data = request.json
        name = data.get('name')
        description = data.get('description', '')
        
        if not name:
            return jsonify({'error': 'Name is required'}), 400
        
        # æ›´æ–°é¡¹ç›®
        db.execute(
            "UPDATE projects SET name = %s, description = %s WHERE id = %s",
            (name, description, id)
        )
        
        return jsonify({'message': 'é¡¹ç›®æ›´æ–°æˆåŠŸ'})
    except Exception as e:
        logger.error(f"æ›´æ–°é¡¹ç›®å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@api.route('/projects/<int:id>', methods=['DELETE'])
def delete_project(id):
    try:
        # åˆ é™¤é¡¹ç›®ç›¸å…³çš„æµ‹è¯•ç”¨ä¾‹
        analysis_results = db.fetch_all("SELECT id FROM analysis_results WHERE project_id = %s", (id,))
        for result in analysis_results:
            db.execute("DELETE FROM test_cases WHERE analysis_id = %s", (result['id'],))
        
        # åˆ é™¤é¡¹ç›®çš„åˆ†æç»“æœ
        db.execute("DELETE FROM analysis_results WHERE project_id = %s", (id,))
        
        # åˆ é™¤é¡¹ç›®
        db.execute("DELETE FROM projects WHERE id = %s", (id,))
        
        return jsonify({'message': 'é¡¹ç›®åˆ é™¤æˆåŠŸ'})
    except Exception as e:
        logger.error(f"åˆ é™¤é¡¹ç›®å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@api.route('/projects/<int:id>/analyze', methods=['POST'])
def analyze_project(id):
    try:
        db = get_db()
        project = db.fetch_one("SELECT * FROM projects WHERE id = %s", (id,))
        
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        data = request.json or {}
        commit_hash = data.get('commit_hash')
        force_update = data.get('force_update', False)
        
        # ä½¿ç”¨Gitå·¥å…·åˆ†æä»“åº“
        git_utils = GitUtils(project['git_url'], project['branch'])
        
        if not git_utils.clone_repo():
            return jsonify({'error': 'æ— æ³•å…‹éš†ä»“åº“ï¼Œè¯·æ£€æŸ¥URLå’Œæƒé™'}), 400
        
        # åˆ†ææäº¤
        analysis_result = git_utils.analyze_commit(commit_hash)
        
        if not analysis_result:
            git_utils.cleanup()
            return jsonify({'error': 'æ— æ³•åˆ†ææäº¤ï¼Œè¯·æ£€æŸ¥æäº¤å“ˆå¸Œæ˜¯å¦æ­£ç¡®'}), 400
        
        # æ›´æ–°é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯
        stats = git_utils.get_repo_stats()
        if stats:
            stats_json = json.dumps(stats)
            db.execute(
                "UPDATE projects SET stats = %s WHERE id = %s",
                (stats_json, id)
            )
        
        # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        test_cases = git_utils.generate_test_cases(analysis_result['changes'])
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        git_utils.cleanup()
        
        # è®¡ç®—é£é™©çº§åˆ«
        risk_level = 'low'
        if analysis_result['complexity_delta'] > 5:
            risk_level = 'high'
        elif analysis_result['complexity_delta'] > 2:
            risk_level = 'medium'
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ç›¸åŒcommit_hashçš„åˆ†æç»“æœ
        existing_analysis = None
        if commit_hash and not force_update:
            existing_analysis = db.fetch_one(
                "SELECT * FROM analysis_results WHERE project_id = %s AND commit_hash = %s",
                (id, commit_hash)
            )
        
        if existing_analysis and not force_update:
            # è¿”å›ç°æœ‰åˆ†æç»“æœ
            existing_result_data = existing_analysis['result_data']
            if isinstance(existing_result_data, str):
                try:
                    existing_result_data = json.loads(existing_result_data)
                except:
                    existing_result_data = {}
            
            # è·å–æµ‹è¯•ç”¨ä¾‹
            existing_test_cases = db.fetch_all(
                "SELECT * FROM test_cases WHERE analysis_id = %s",
                (existing_analysis['id'],)
            )
            
            # å‘é€WebSocketé€šçŸ¥
            send_notification('analysis_complete', {
                'analysisId': existing_analysis['id'],
                'projectId': id,
                'message': 'ä½¿ç”¨ç°æœ‰åˆ†æç»“æœ'
            })
            
            return jsonify({
                'message': 'ä½¿ç”¨ç°æœ‰åˆ†æç»“æœ',
                'analysis_id': existing_analysis['id'],
                'change_details': existing_result_data.get('result', {}),
                'test_cases': existing_test_cases
            })
        
        # ä¿å­˜åˆ†æç»“æœ
        result_data = {
            'analysis_type': 'code_change',
            'commit_hash': analysis_result['commit_hash'],
            'result': analysis_result,
            'risk_level': risk_level
        }
        
        # æ’å…¥åˆ†æç»“æœ
        result_data_json = json.dumps(result_data)
        analysis_id = db.execute(
            """
            INSERT INTO analysis_results 
            (project_id, analysis_type, commit_hash, result_data, risk_level, created_at) 
            VALUES (%s, %s, %s, %s, %s, %s) 
            RETURNING id
            """,
            (
                id, 
                'code_change', 
                analysis_result['commit_hash'], 
                result_data_json, 
                risk_level, 
                datetime.now()
            )
        )
        
        # æ’å…¥æµ‹è¯•ç”¨ä¾‹
        for test_case in test_cases:
            db.execute(
                """
                INSERT INTO test_cases 
                (analysis_id, name, test_type, priority, test_code, created_at) 
                VALUES (%s, %s, %s, %s, %s, %s)
                """,
                (
                    analysis_id['id'], 
                    test_case['name'], 
                    test_case['type'], 
                    test_case['priority'], 
                    test_case['code'], 
                    datetime.now()
                )
            )
        
        # å‘é€WebSocketé€šçŸ¥
        send_notification('analysis_complete', {
            'analysisId': analysis_id['id'],
            'projectId': id,
            'message': 'ä»£ç å˜æ›´åˆ†æå®Œæˆ'
        })
        
        # è¿”å›åˆ†æç»“æœå’Œæµ‹è¯•ç”¨ä¾‹
        return jsonify({
            'message': 'ä»£ç å˜æ›´åˆ†æå®Œæˆ',
            'analysis_id': analysis_id['id'],
            'change_details': analysis_result,
            'test_cases': test_cases
        })
    except Exception as e:
        logger.error(f"åˆ†æé¡¹ç›®å˜æ›´å¤±è´¥: {str(e)}")
        
        # å‘é€å¤±è´¥é€šçŸ¥
        send_notification('analysis_failed', {
            'projectId': id,
            'error': str(e)
        })
        
        return jsonify({'error': str(e)}), 500

@api.route('/projects/validate-repo', methods=['POST'])
def validate_repo():
    try:
        data = request.json
        repo_url = data.get('repoUrl')
        
        if not repo_url:
            return jsonify({'error': 'Repository URL is required'}), 400
        
        # éªŒè¯ä»“åº“
        git_utils = GitUtils(repo_url)
        if git_utils.validate_repo():
            return jsonify({'message': 'ä»“åº“éªŒè¯æˆåŠŸ'})
        else:
            return jsonify({'error': 'ä»“åº“éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥URLæ˜¯å¦æ­£ç¡®å¹¶ç¡®ä¿ä»“åº“å¯è®¿é—®'}), 400
    except Exception as e:
        logger.error(f"éªŒè¯ä»“åº“å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@api.route('/projects/repo-branches', methods=['GET'])
def get_repo_branches():
    try:
        repo_url = request.args.get('repoUrl')
        if not repo_url:
            return jsonify({'error': 'Repository URL is required'}), 400
        
        # è·å–ä»“åº“åˆ†æ”¯
        git_utils = GitUtils(repo_url)
        branches = git_utils.get_branches()
        
        return jsonify(branches)
    except Exception as e:
        logger.error(f"è·å–ä»“åº“åˆ†æ”¯å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@api.route('/projects/<int:id>/index', methods=['POST'])
def index_project(id):
    """ä¸ºé¡¹ç›®åˆ›å»ºä»£ç ç´¢å¼•"""
    try:
        db = get_db()
        project = db.fetch_one("SELECT * FROM projects WHERE id = %s", (id,))
        
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        # ä½¿ç”¨Gitå·¥å…·è·å–ä»£ç åº“è·¯å¾„
        git_utils = GitUtils(project['git_url'], project['branch'])
        
        if not git_utils.clone_repo():
            return jsonify({'error': 'æ— æ³•å…‹éš†ä»“åº“ï¼Œè¯·æ£€æŸ¥URLå’Œæƒé™'}), 400
        
        # è·å–ä»“åº“æœ¬åœ°è·¯å¾„
        repo_path = git_utils.temp_dir
        
        # åˆ›å»ºä»£ç ç´¢å¼•
        indexer = CodebaseIndexer(index_dir=os.path.join(repo_path, ".code_index"))
        index_result = indexer.build_index(repo_path)
        
        # ä¿å­˜ç´¢å¼•ç»“æœåˆ°æ•°æ®åº“
        index_data = {
            "symbol_count": index_result["symbol_count"],
            "module_count": index_result["module_count"],
            "index_path": index_result["index_path"]
        }
        
        # å°†ç´¢å¼•ä¿¡æ¯ä¿å­˜åˆ°é¡¹ç›®ä¸­
        db.execute(
            "UPDATE projects SET code_index = %s WHERE id = %s",
            (json.dumps(index_data), id)
        )
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        git_utils.cleanup()
        
        return jsonify({
            'message': 'ä»£ç ç´¢å¼•åˆ›å»ºæˆåŠŸ',
            'index_stats': index_data
        })
    except Exception as e:
        logger.error(f"åˆ›å»ºä»£ç ç´¢å¼•å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@api.route('/projects/<int:id>/search', methods=['GET'])
def search_code(id):
    """åœ¨é¡¹ç›®ä»£ç ä¸­æœç´¢"""
    try:
        db = get_db()
        project = db.fetch_one("SELECT * FROM projects WHERE id = %s", (id,))
        
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        query = request.args.get('q')
        if not query:
            return jsonify({'error': 'æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º'}), 400
        
        # æ£€æŸ¥é¡¹ç›®æ˜¯å¦æœ‰ä»£ç ç´¢å¼•
        if not project.get('code_index'):
            # å¦‚æœæ²¡æœ‰ç´¢å¼•ï¼Œå…ˆåˆ›å»ºç´¢å¼•
            return jsonify({'error': 'é¡¹ç›®å°šæœªåˆ›å»ºä»£ç ç´¢å¼•ï¼Œè¯·å…ˆè°ƒç”¨ç´¢å¼•API'}), 400
        
        # ä½¿ç”¨Gitå·¥å…·è·å–ä»£ç åº“è·¯å¾„
        git_utils = GitUtils(project['git_url'], project['branch'])
        
        if not git_utils.clone_repo():
            return jsonify({'error': 'æ— æ³•å…‹éš†ä»“åº“ï¼Œè¯·æ£€æŸ¥URLå’Œæƒé™'}), 400
        
        # è·å–ä»“åº“æœ¬åœ°è·¯å¾„
        repo_path = git_utils.temp_dir
        
        # åŠ è½½ä»£ç ç´¢å¼•
        indexer = CodebaseIndexer(index_dir=os.path.join(repo_path, ".code_index"))
        if not indexer.load_index():
            # å¦‚æœåŠ è½½å¤±è´¥ï¼Œé‡æ–°åˆ›å»ºç´¢å¼•
            indexer.build_index(repo_path)
        
        # æ‰§è¡Œæœç´¢
        top_k = int(request.args.get('limit', 10))
        results = indexer.find_similar_symbols(query, top_k=top_k)
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        git_utils.cleanup()
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for result in results:
            symbol = result['symbol']
            formatted_results.append({
                'name': symbol.name,
                'type': symbol.symbol_type,
                'module': result['module_path'],
                'line': symbol.line,
                'column': symbol.column,
                'parameters': symbol.parameters,
                'return_type': symbol.return_type,
                'score': result['similarity_score']
            })
        
        return jsonify({
            'query': query,
            'results': formatted_results
        })
    except Exception as e:
        logger.error(f"ä»£ç æœç´¢å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@api.route('/projects/<int:id>/test-cases', methods=['GET'])
def get_test_cases(id):
    """è·å–é¡¹ç›®çš„æµ‹è¯•ç”¨ä¾‹"""
    try:
        db = get_db()
        project = db.fetch_one("SELECT * FROM projects WHERE id = %s", (id,))
        
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        # é¦–å…ˆå°è¯•ä»åŸºäºç´¢å¼•çš„åˆ†æç»“æœä¸­è·å–åŠŸèƒ½æµ‹è¯•
        try:
            latest_analysis = db.fetch_one(
                "SELECT analysis_data FROM code_analysis WHERE project_id = %s AND analysis_type = 'index_based' ORDER BY created_at DESC LIMIT 1",
                (id,)
            )
            
            if latest_analysis and latest_analysis['analysis_data']:
                analysis_data = json.loads(latest_analysis['analysis_data'])
                test_recommendations = analysis_data.get('test_recommendations', {})
                functional_tests = test_recommendations.get('functional_tests', [])
                
                if functional_tests:
                    logger.info(f"è¿”å› {len(functional_tests)} ä¸ªåŠŸèƒ½æµ‹è¯•ç”¨ä¾‹")
                    # è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
                    formatted_tests = []
                    for i, test in enumerate(functional_tests):
                        formatted_test = {
                            'id': i + 1,
                            'name': test.get('name', 'æœªå‘½åæµ‹è¯•'),
                            'description': test.get('description', ''),
                            'test_type': 'functional',
                            'priority': test.get('priority', 'medium'),
                            'test_scenarios': test.get('test_scenarios', []),
                            'test_data_requirements': test.get('test_data_requirements', []),
                            'expected_outcomes': test.get('expected_outcomes', []),
                            'estimated_time': test.get('estimated_time', 10),
                            'created_at': datetime.now().isoformat(),
                            'generation_method': 'index_based_functional'
                        }
                        formatted_tests.append(formatted_test)
                    
                    return jsonify(formatted_tests)
        except Exception as e:
            logger.warning(f"è·å–åŸºäºç´¢å¼•çš„åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰åŸºäºç´¢å¼•çš„åˆ†æï¼Œå›é€€åˆ°æ—§çš„é€»è¾‘ä½†è¿”å›åŠŸèƒ½æµ‹è¯•æ ¼å¼
        latest_analysis = db.fetch_one(
            "SELECT id, result_data FROM analysis_results WHERE project_id = %s ORDER BY created_at DESC LIMIT 1", 
            (id,)
        )
        
        analysis_id = latest_analysis['id'] if latest_analysis else None
        
        # è·å–ç°æœ‰çš„æµ‹è¯•ç”¨ä¾‹
        if analysis_id:
            analysis_tests = db.fetch_all(
                "SELECT * FROM test_cases WHERE analysis_id = %s",
                (analysis_id,)
            )
        else:
            analysis_tests = []
        
        # å¦‚æœæ²¡æœ‰æµ‹è¯•ç”¨ä¾‹ï¼Œå°è¯•ç”Ÿæˆæ™ºèƒ½åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹
        if not analysis_tests:
            logger.info(f"ğŸ§  å°è¯•ç”Ÿæˆæ™ºèƒ½åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹ for project {id}")
            
            # å°è¯•ä½¿ç”¨å¢å¼ºAIå®¢æˆ·ç«¯ç”Ÿæˆæ™ºèƒ½æµ‹è¯•ç”¨ä¾‹
            try:
                from clients.enhanced_ai_client import EnhancedAIClient
                
                # è·å–é¡¹ç›®ä¿¡æ¯
                project = db.fetch_one("SELECT * FROM projects WHERE id = ?", (id,))
                if project:
                    project_path = project.get('git_url', '')
                    project_name = project.get('name', f'Project {id}')
                    
                    # åˆ›å»ºAIå®¢æˆ·ç«¯
                    ai_client = EnhancedAIClient()
                    
                    # ç”Ÿæˆæ™ºèƒ½åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹ - ä¿®å¤å¼‚æ­¥è°ƒç”¨é—®é¢˜
                    import asyncio
                    try:
                        # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯è€Œä¸æ˜¯è·å–ç°æœ‰çš„
                        try:
                            loop = asyncio.get_event_loop()
                            if loop.is_running():
                                # å¦‚æœå½“å‰çº¿ç¨‹å·²æœ‰è¿è¡Œçš„å¾ªç¯ï¼Œä½¿ç”¨asyncio.run
                                import concurrent.futures
                                with concurrent.futures.ThreadPoolExecutor() as executor:
                                    future = executor.submit(
                                        asyncio.run,
                                        ai_client.generate_comprehensive_functional_tests(
                                            change_analysis={
                                                'project_path': project_path,
                                                'project_name': project_name,
                                                'project_id': id
                                            },
                                            system_context={
                                                'project_type': 'web_application',
                                                'tech_stack': ['python', 'javascript'],
                                                'user_roles': ['admin', 'user']
                                            }
                                        )
                                    )
                                    intelligent_tests = future.result(timeout=30)
                            else:
                                intelligent_tests = loop.run_until_complete(
                                    ai_client.generate_comprehensive_functional_tests(
                                        change_analysis={
                                            'project_path': project_path,
                                            'project_name': project_name,
                                            'project_id': id
                                        },
                                        system_context={
                                            'project_type': 'web_application',
                                            'tech_stack': ['python', 'javascript'],
                                            'user_roles': ['admin', 'user']
                                        }
                                    )
                                )
                        except RuntimeError:
                            # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
                            intelligent_tests = asyncio.run(
                                ai_client.generate_comprehensive_functional_tests(
                                    change_analysis={
                                        'project_path': project_path,
                                        'project_name': project_name,
                                        'project_id': id
                                    },
                                    system_context={
                                        'project_type': 'web_application',
                                        'tech_stack': ['python', 'javascript'],
                                        'user_roles': ['admin', 'user']
                                    }
                                )
                            )
                    except Exception as async_error:
                        logger.warning(f"å¼‚æ­¥è°ƒç”¨å¤±è´¥: {async_error}")
                        # å›é€€åˆ°åŒæ­¥æ–¹å¼ç”Ÿæˆé»˜è®¤ç”¨ä¾‹
                        intelligent_tests = None
                    
                    if intelligent_tests and isinstance(intelligent_tests, (list, dict)) and len(intelligent_tests) > 0:
                        logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(intelligent_tests) if isinstance(intelligent_tests, list) else 1} ä¸ªæ™ºèƒ½åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹")
                        return jsonify(intelligent_tests)
                        
            except Exception as e:
                logger.warning(f"æ™ºèƒ½æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆå¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤æµ‹è¯•ç”¨ä¾‹")
            
            logger.info(f"ç”Ÿæˆé»˜è®¤åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹ for project {id}")
            
            # ç”ŸæˆåŸºäºé¡¹ç›®çš„åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹
            default_functional_tests = [
                {
                    'id': 1,
                    'name': 'åŠŸèƒ½æµ‹è¯• - é¡¹ç›®æ ¸å¿ƒåŠŸèƒ½éªŒè¯',
                    'description': 'éªŒè¯é¡¹ç›®çš„æ ¸å¿ƒåŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ',
                    'test_type': 'functional',
                    'priority': 'high',
                    'test_scenarios': [
                        'æµ‹è¯•é¡¹ç›®åˆå§‹åŒ–æµç¨‹',
                        'æµ‹è¯•ä¸»è¦åŠŸèƒ½æ¨¡å—çš„æ­£å¸¸è¿è¡Œ',
                        'æµ‹è¯•å¼‚å¸¸æƒ…å†µçš„å¤„ç†æœºåˆ¶',
                        'æµ‹è¯•ç”¨æˆ·ç•Œé¢çš„å“åº”æ€§'
                    ],
                    'test_data_requirements': [
                        'æœ‰æ•ˆçš„è¾“å…¥æ•°æ®é›†',
                        'è¾¹ç•Œå€¼æµ‹è¯•æ•°æ®',
                        'å¼‚å¸¸è¾“å…¥æ•°æ®',
                        'æ€§èƒ½æµ‹è¯•æ•°æ®'
                    ],
                    'expected_outcomes': [
                        'æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½åº”æ­£å¸¸æ‰§è¡Œ',
                        'å¼‚å¸¸æƒ…å†µåº”æœ‰åˆé€‚çš„é”™è¯¯å¤„ç†',
                        'ç”¨æˆ·ç•Œé¢åº”å“åº”æµç•…',
                        'æ•°æ®å¤„ç†åº”å‡†ç¡®æ— è¯¯'
                    ],
                    'estimated_time': 30,
                    'created_at': datetime.now().isoformat(),
                    'generation_method': 'default_functional'
                },
                {
                    'id': 2,
                    'name': 'åŠŸèƒ½æµ‹è¯• - æ•°æ®å¤„ç†éªŒè¯',
                    'description': 'éªŒè¯æ•°æ®è¾“å…¥ã€å¤„ç†å’Œè¾“å‡ºçš„å®Œæ•´æ€§',
                    'test_type': 'functional',
                    'priority': 'medium',
                    'test_scenarios': [
                        'æµ‹è¯•æ•°æ®è¾“å…¥éªŒè¯æœºåˆ¶',
                        'æµ‹è¯•æ•°æ®å¤„ç†é€»è¾‘çš„æ­£ç¡®æ€§',
                        'æµ‹è¯•æ•°æ®è¾“å‡ºæ ¼å¼çš„ä¸€è‡´æ€§',
                        'æµ‹è¯•å¤§é‡æ•°æ®çš„å¤„ç†æ€§èƒ½'
                    ],
                    'test_data_requirements': [
                        'æ ‡å‡†æ ¼å¼çš„æµ‹è¯•æ•°æ®',
                        'ä¸åŒç±»å‹çš„è¾“å…¥æ•°æ®',
                        'å¤§å®¹é‡æ•°æ®é›†',
                        'æ ¼å¼é”™è¯¯çš„æ•°æ®æ ·æœ¬'
                    ],
                    'expected_outcomes': [
                        'è¾“å…¥éªŒè¯åº”æ­£ç¡®è¯†åˆ«æœ‰æ•ˆå’Œæ— æ•ˆæ•°æ®',
                        'æ•°æ®å¤„ç†åº”ä¿æŒå‡†ç¡®æ€§å’Œä¸€è‡´æ€§',
                        'è¾“å‡ºæ ¼å¼åº”ç¬¦åˆé¢„æœŸè§„èŒƒ',
                        'æ€§èƒ½åº”åœ¨å¯æ¥å—èŒƒå›´å†…'
                    ],
                    'estimated_time': 25,
                    'created_at': datetime.now().isoformat(),
                    'generation_method': 'default_functional'
                },
                {
                    'id': 3,
                    'name': 'åŠŸèƒ½æµ‹è¯• - ç”¨æˆ·äº¤äº’éªŒè¯',
                    'description': 'éªŒè¯ç”¨æˆ·ç•Œé¢å’Œäº¤äº’åŠŸèƒ½çš„å¯ç”¨æ€§',
                    'test_type': 'functional',
                    'priority': 'medium',
                    'test_scenarios': [
                        'æµ‹è¯•ç”¨æˆ·ç•Œé¢å…ƒç´ çš„å¯è®¿é—®æ€§',
                        'æµ‹è¯•ç”¨æˆ·æ“ä½œçš„å“åº”é€Ÿåº¦',
                        'æµ‹è¯•é”™è¯¯æç¤ºçš„å‡†ç¡®æ€§',
                        'æµ‹è¯•ä¸åŒæµè§ˆå™¨çš„å…¼å®¹æ€§'
                    ],
                    'test_data_requirements': [
                        'ä¸åŒç”¨æˆ·è§’è‰²çš„æµ‹è¯•è´¦å·',
                        'å„ç§æ“ä½œåœºæ™¯çš„æµ‹è¯•è„šæœ¬',
                        'ä¸åŒæµè§ˆå™¨ç¯å¢ƒ',
                        'ç½‘ç»œçŠ¶å†µæ¨¡æ‹Ÿæ•°æ®'
                    ],
                    'expected_outcomes': [
                        'ç•Œé¢å…ƒç´ åº”æ­£ç¡®æ˜¾ç¤ºå’Œå“åº”',
                        'ç”¨æˆ·æ“ä½œåº”å¾—åˆ°åŠæ—¶åé¦ˆ',
                        'é”™è¯¯ä¿¡æ¯åº”æ¸…æ™°æ˜ç¡®',
                        'åº”æ”¯æŒä¸»æµæµè§ˆå™¨'
                    ],
                    'estimated_time': 20,
                    'created_at': datetime.now().isoformat(),
                    'generation_method': 'default_functional'
                }
            ]
            
            return jsonify(default_functional_tests)
        
        # è½¬æ¢ç°æœ‰æµ‹è¯•ç”¨ä¾‹ä¸ºåŠŸèƒ½æµ‹è¯•æ ¼å¼
        formatted_tests = []
        for test in analysis_tests:
            formatted_test = {
                'id': test.get('id'),
                'name': test.get('name', 'æœªå‘½åæµ‹è¯•'),
                'description': test.get('description', ''),
                'test_type': 'functional',
                'priority': test.get('priority', 'medium'),
                'test_scenarios': [
                    f"éªŒè¯{test.get('name', 'åŠŸèƒ½')}çš„åŸºæœ¬æ“ä½œ",
                    f"æµ‹è¯•{test.get('name', 'åŠŸèƒ½')}çš„è¾¹ç•Œæ¡ä»¶",
                    f"æ£€æŸ¥{test.get('name', 'åŠŸèƒ½')}çš„é”™è¯¯å¤„ç†"
                ],
                'test_data_requirements': [
                    'æ­£å¸¸è¾“å…¥æ•°æ®',
                    'è¾¹ç•Œå€¼æ•°æ®',
                    'å¼‚å¸¸è¾“å…¥æ•°æ®'
                ],
                'expected_outcomes': [
                    f"{test.get('name', 'åŠŸèƒ½')}åº”æ­£å¸¸æ‰§è¡Œå¹¶è¿”å›é¢„æœŸç»“æœ"
                ],
                'estimated_time': 15,
                'created_at': test.get('created_at', datetime.now().isoformat()),
                'generation_method': 'converted_from_old'
            }
            formatted_tests.append(formatted_test)
        
        logger.info(f"è¿”å› {len(formatted_tests)} ä¸ªè½¬æ¢åçš„åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹")
        return jsonify(formatted_tests)
        
    except Exception as e:
        logger.error(f"è·å–æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@api.route('/analyzer/analyze', methods=['POST'])
async def analyze_code():
    """åˆ†æä»£ç å˜æ›´æ¥å£"""
    data = request.json
    if not data:
        return jsonify({"error": "Missing request data"}), 400
    
    repo_path = data.get('repo_path')
    if not repo_path:
        return jsonify({"error": "Missing repository path"}), 400
    
    commit_hash = data.get('commit_hash')
    config = data.get('config', {})
    
    try:
        # åˆ›å»ºåˆ†æå™¨å¹¶æ‰§è¡Œåˆ†æ
        analyzer = EnhancedCursorAnalyzer(repo_path, config)
        results = await analyzer.analyze_repository_changes(commit_hash)
        
        return jsonify(results.to_dict())
    except Exception as e:
        logger.exception("åˆ†æä»£ç æ—¶å‡ºé”™")
        return jsonify({"status": "error", "message": str(e)}), 500

@api.route('/analyzer/ai-providers', methods=['GET'])
async def get_ai_providers():
    """è·å–å¯ç”¨AIæœåŠ¡æä¾›å•†"""
    try:
        providers = {
            "openai": {"name": "OpenAI", "models": ["gpt-3.5-turbo", "gpt-4"]},
            "anthropic": {"name": "Anthropic", "models": ["claude-3-sonnet", "claude-3-opus"]},
            "local": {"name": "Local LLM", "models": ["llama2", "mistral"]},
            "azure": {"name": "Azure OpenAI", "models": ["gpt-35-turbo", "gpt-4"]},
            "google": {"name": "Google AI", "models": ["gemini-pro"]}
        }
        
        return jsonify({
            "status": "success",
            "providers": providers
        })
    except Exception as e:
        logger.exception("è·å–AIæä¾›å•†æ—¶å‡ºé”™")
        return jsonify({"status": "error", "message": str(e)}), 500

@api.route('/analyzer/test-generator', methods=['POST'])
async def generate_tests():
    """ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æ¥å£"""
    data = request.json
    if not data:
        return jsonify({"error": "Missing request data"}), 400
    
    repo_path = data.get('repo_path')
    file_path = data.get('file_path')
    function_name = data.get('function_name')
    language = data.get('language')
    config = data.get('config', {})
    
    if not repo_path or not file_path:
        return jsonify({"error": "Missing required parameters"}), 400
    
    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = EnhancedCursorAnalyzer(repo_path, config)
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = analyzer._read_file_content(file_path)
        
        if not language:
            language = analyzer.parser.get_language_from_file(file_path)
            if not language:
                return jsonify({"error": "Unsupported file type"}), 400
        
        if function_name:
            # ç”Ÿæˆç‰¹å®šå‡½æ•°çš„æµ‹è¯•
            function_code = analyzer._extract_function_code(content, function_name, language)
            if not function_code:
                return jsonify({"error": f"Function {function_name} not found"}), 400
            
            test_code = await analyzer.ai_integrator.generate_test_cases(function_code, language)
            return jsonify({
                "status": "success",
                "tests": [{
                    "name": f"test_{function_name}",
                    "target_function": function_name,
                    "test_code": test_code or analyzer._generate_default_test_code(function_name, language),
                    "language": language
                }]
            })
        else:
            # æå–æ‰€æœ‰å‡½æ•°å¹¶ç”Ÿæˆæµ‹è¯•
            functions, _ = analyzer.parser.extract_functions_and_classes(content, language)
            
            tests = []
            for func in functions[:5]:  # é™åˆ¶ä¸ºå‰5ä¸ªå‡½æ•°ï¼Œé¿å…è¯·æ±‚è¿‡å¤š
                function_code = analyzer._extract_function_code(content, func, language)
                if function_code:
                    test_code = await analyzer.ai_integrator.generate_test_cases(function_code, language)
                    tests.append({
                        "name": f"test_{func}",
                        "target_function": func,
                        "test_code": test_code or analyzer._generate_default_test_code(func, language),
                        "language": language
                    })
            
            return jsonify({
                "status": "success",
                "tests": tests
            })
            
    except Exception as e:
        logger.exception("ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æ—¶å‡ºé”™")
        return jsonify({"status": "error", "message": str(e)}), 500

@api.route('/analyzer/suggest-improvements', methods=['POST'])
async def suggest_improvements():
    """å»ºè®®ä»£ç æ”¹è¿›æ¥å£"""
    data = request.json
    if not data:
        return jsonify({"error": "Missing request data"}), 400
    
    code = data.get('code')
    language = data.get('language')
    config = data.get('config', {})
    
    if not code or not language:
        return jsonify({"error": "Missing code or language"}), 400
    
    try:
        # ä½¿ç”¨ä¸´æ—¶ä»“åº“è·¯å¾„
        temp_repo_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
        # åˆ›å»ºåˆ†æå™¨
        analyzer = EnhancedCursorAnalyzer(temp_repo_path, config)
        
        # è·å–æ”¹è¿›å»ºè®®
        suggestions = await analyzer.ai_integrator.suggest_improvements(code, language)
        
        return jsonify({
            "status": "success",
            "suggestions": suggestions
        })
    except Exception as e:
        logger.exception("è·å–ä»£ç æ”¹è¿›å»ºè®®æ—¶å‡ºé”™")
        return jsonify({"status": "error", "message": str(e)}), 500

@api.route('/analyzer/config-template', methods=['GET'])
def get_config_template():
    """è·å–åˆ†æé…ç½®æ¨¡æ¿"""
    try:
        template = {
            "ai_services": {
                "openai": {
                    "api_key": "",
                    "model": "gpt-4",
                    "enabled": False
                },
                "anthropic": {
                    "api_key": "",
                    "model": "claude-3-sonnet-20240229",
                    "enabled": False
                },
                "azure_openai": {
                    "api_key": "",
                    "endpoint": "",
                    "deployment_name": "",
                    "enabled": False
                },
                "google_ai": {
                    "api_key": "",
                    "model": "gemini-pro",
                    "enabled": False
                }
            },
            "analysis": {
                "max_complexity_threshold": 10,
                "risk_assessment_enabled": True,
                "dependency_analysis_enabled": True,
                "include_code_snippets": True,
                "max_suggestions": 10
            },
            "output": {
                "detailed_impact_analysis": True,
                "generate_test_suggestions": True,
                "include_business_impact": True
            }
        }
        
        return jsonify({
            "status": "success",
            "config_template": template
        })
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@api.route('/analyzer/validate-config', methods=['POST'])
def validate_config():
    """éªŒè¯åˆ†æé…ç½®"""
    try:
        config = request.get_json()
        
        # åŸºæœ¬é…ç½®éªŒè¯
        if not config:
            return jsonify({
                "status": "error",
                "message": "é…ç½®ä¸èƒ½ä¸ºç©º"
            }), 400
        
        # éªŒè¯AIæœåŠ¡é…ç½®
        ai_services = config.get('ai_services', {})
        enabled_services = []
        
        for service_name, service_config in ai_services.items():
            if service_config.get('enabled', False):
                if not service_config.get('api_key'):
                    return jsonify({
                        "status": "error",
                        "message": f"{service_name} å·²å¯ç”¨ä½†ç¼ºå°‘APIå¯†é’¥"
                    }), 400
                enabled_services.append(service_name)
        
        return jsonify({
            "status": "success",
            "message": "é…ç½®éªŒè¯é€šè¿‡",
            "enabled_services": enabled_services
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@api.route('/analyzer/task/<task_id>/status', methods=['GET'])
def get_task_status(task_id):
    """è·å–åˆ†æä»»åŠ¡çŠ¶æ€"""
    try:
        # è¿™é‡Œåº”è¯¥æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€æ•°æ®åº“æˆ–ç¼“å­˜
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
        return jsonify({
            "status": "success",
            "task_id": task_id,
            "progress": 100,
            "status_text": "åˆ†æå®Œæˆ",
            "completed": True,
            "failed": False,
            "result": {
                "status": "success",
                "changes_count": 3,
                "analysis_results": [],
                "global_test_strategy": None,
                "summary": {
                    "total_changes": 3,
                    "high_risk_changes": 1,
                    "medium_risk_changes": 1,
                    "low_risk_changes": 1,
                    "total_suggested_tests": 8,
                    "overall_recommendation": "å»ºè®®ä¼˜å…ˆå¤„ç†é«˜é£é™©å˜æ›´ï¼Œæ‰§è¡Œå…¨éƒ¨å»ºè®®æµ‹è¯•"
                }
            }
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@api.route('/analyzer/task/<task_id>/cancel', methods=['POST'])
def cancel_task(task_id):
    """å–æ¶ˆä»»åŠ¡"""
    try:
        # è¿™é‡Œåº”è¯¥å®ç°ä»»åŠ¡å–æ¶ˆé€»è¾‘
        # æš‚æ—¶è¿”å›æˆåŠŸçŠ¶æ€
        return jsonify({
            "status": "success",
            "message": f"ä»»åŠ¡ {task_id} å·²å–æ¶ˆ"
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500



@api.route('/analyzer/analysis/<analysis_id>', methods=['GET'])
def get_analysis_details(analysis_id):
    """è·å–ç‰¹å®šåˆ†æç»“æœçš„è¯¦æƒ…"""
    try:
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“è·å–åˆ†æç»“æœ
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
        analysis_result = {
            "id": analysis_id,
            "status": "success",
            "changes_count": 3,
            "analysis_results": [
                {
                    "change": {
                        "file_path": "src/main.py",
                        "change_type": "modified",
                        "affected_functions": ["main", "process_data"],
                        "affected_classes": ["DataProcessor"],
                        "complexity_delta": 2.5,
                        "risk_level": "medium",
                        "business_impact": ["core_logic", "performance"]
                    },
                    "impact": {
                        "direct_impacts": ["src/utils.py", "src/models.py"],
                        "indirect_impacts": ["tests/test_main.py"],
                        "risk_factors": ["high_complexity", "critical_path"],
                        "suggested_tests": [
                            {
                                "name": "test_main_function",
                                "test_type": "unit",
                                "priority": "high",
                                "coverage_areas": ["main", "process_data"]
                            }
                        ],
                        "confidence_score": 0.85
                    }
                }
            ],
            "global_test_strategy": {
                "priority_tests": [],
                "unit_tests": [],
                "integration_tests": [],
                "e2e_tests": [],
                "estimated_coverage": 0.75,
                "estimated_time": 30,
                "recommendations": []
            },
            "summary": {
                "total_changes": 3,
                "high_risk_changes": 1,
                "medium_risk_changes": 1,
                "low_risk_changes": 1,
                "total_suggested_tests": 8,
                "overall_recommendation": "å»ºè®®ä¼˜å…ˆå¤„ç†é«˜é£é™©å˜æ›´ï¼Œæ‰§è¡Œå…¨éƒ¨å»ºè®®æµ‹è¯•"
            }
        }
        
        return jsonify({
            "status": "success",
            "analysis": analysis_result
        })
        
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@api.route('/analyzer/analysis/<analysis_id>/export', methods=['GET'])
def export_analysis_report(analysis_id):
    """å¯¼å‡ºåˆ†ææŠ¥å‘Š"""
    try:
        format_type = request.args.get('format', 'html')
        
        # è·å–åˆ†æç»“æœ
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“è·å–å®é™…æ•°æ®
        
        if format_type == 'html':
            # ç”ŸæˆHTMLæŠ¥å‘Š
            reporter = HTMLReporter()
            
            # è¿™é‡Œåº”è¯¥ä¼ å…¥å®é™…çš„åˆ†æç»“æœ
            html_content = reporter._get_html_template().render(
                title='åˆ†ææŠ¥å‘Š',
                generated_time='2024-01-15 10:30:00',
                result={
                    'status': 'success',
                    'changes_count': 3
                }
            )
            
            return Response(
                html_content,
                mimetype='text/html',
                headers={
                    'Content-Disposition': f'attachment; filename=analysis_{analysis_id}.html'
                }
            )
            
        elif format_type == 'json':
            # ç”ŸæˆJSONæŠ¥å‘Š
            reporter = JSONReporter()
            
            # è¿™é‡Œåº”è¯¥ä¼ å…¥å®é™…çš„åˆ†æç»“æœ
            json_data = {
                "status": "success",
                "analysis_id": analysis_id,
                "generated_time": "2024-01-15T10:30:00Z"
            }
            
            return jsonify(json_data)
            
        else:
            return jsonify({
                "status": "error",
                "message": f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format_type}"
            }), 400
            
    except Exception as e:
        return jsonify({
            "status": "error",
            "message": str(e)
        }), 500

@api.route('/projects/<int:id>/analysis', methods=['GET'])
def get_project_analysis(id):
    """è·å–é¡¹ç›®çš„åˆ†æç»“æœ"""
    try:
        db = get_db()
        project = db.fetch_one("SELECT * FROM projects WHERE id = %s", (id,))
        
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        # è·å–æœ€æ–°çš„åˆ†æç»“æœ
        latest_analysis = db.fetch_one(
            """
            SELECT * FROM analysis_results 
            WHERE project_id = %s 
            ORDER BY created_at DESC 
            LIMIT 1
            """, 
            (id,)
        )
        
        if not latest_analysis:
            return jsonify({
                'status': 'no_analysis',
                'message': 'è¯¥é¡¹ç›®è¿˜æ²¡æœ‰åˆ†æç»“æœ',
                'project_id': id,
                'project_name': project.get('name', ''),
                'analysis_data': None
            })
        
        # å¤„ç†åˆ†æç»“æœæ•°æ®
        result_data = latest_analysis['result_data']
        if isinstance(result_data, str):
            try:
                result_data = json.loads(result_data)
            except:
                result_data = {}
        
        # è·å–è¯¥åˆ†æçš„æµ‹è¯•ç”¨ä¾‹
        test_cases = db.fetch_all(
            "SELECT * FROM test_cases WHERE analysis_id = %s ORDER BY priority DESC",
            (latest_analysis['id'],)
        )
        
        # æ„å»ºåˆ†æç»“æœå“åº”
        analysis_response = {
            'status': 'success',
            'project_id': id,
            'project_name': project.get('name', ''),
            'analysis_id': latest_analysis['id'],
            'analysis_type': latest_analysis.get('analysis_type', 'code_change'),
            'created_at': latest_analysis['created_at'],
            'risk_level': latest_analysis.get('risk_level', 'low'),
            'analysis_data': result_data,
            'test_cases': test_cases,
            'commit_hash': latest_analysis.get('commit_hash'),
            
            # å…¼å®¹å‰ç«¯æœŸæœ›çš„æ•°æ®ç»“æ„
            'totalFiles': result_data.get('result', {}).get('files_changed', 0),
            'totalLines': 0,  # å¯ä»¥ä»statsè®¡ç®—
            'codeLines': 0,
            'commentLines': 0,
            'emptyLines': 0,
            'complexityScore': result_data.get('result', {}).get('complexity_delta', 0),
            
            # æ–‡ä»¶ç±»å‹åˆ†å¸ƒï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼Œå¯ä»¥ä»å®é™…åˆ†æä¸­æå–ï¼‰
            'fileTypeDistribution': [
                {'name': 'Python', 'value': 5},
                {'name': 'JavaScript', 'value': 3},
                {'name': 'HTML', 'value': 2},
                {'name': 'CSS', 'value': 1}
            ],
            
            # å¤æ‚åº¦è¶‹åŠ¿ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼‰
            'complexityTrend': {
                'dates': ['2024-01-01', '2024-01-02', '2024-01-03'],
                'scores': [2.5, 3.0, result_data.get('result', {}).get('complexity_delta', 2.8)]
            },
            
            # ç¬¦å·åˆ†æï¼ˆä»å˜æ›´ä¸­æå–ï¼‰
            'symbols': [],
            
            # è¯­ä¹‰å˜æ›´åˆ†æ
            'semanticChanges': []
        }
        
        # ä»åˆ†æç»“æœä¸­æå–ç¬¦å·ä¿¡æ¯å’Œè¯­ä¹‰å˜æ›´
        analysis_result = result_data.get('result', {})
        if analysis_result and 'changes' in analysis_result:
            for change in analysis_result['changes']:
                # æ·»åŠ ç¬¦å·ä¿¡æ¯
                analysis_response['symbols'].append({
                    'name': change.get('file', 'Unknown'),
                    'type': change.get('file_type', 'Unknown'),
                    'file': change.get('file', ''),
                    'line': 1,
                    'complexity': change.get('complexity', 0)
                })
                
                # æ·»åŠ è¯­ä¹‰å˜æ›´
                analysis_response['semanticChanges'].append({
                    'type': change.get('type', 'modified'),
                    'description': change.get('changes', ''),
                    'impact': f"å½±å“ {change.get('insertions', 0)} è¡Œæ–°å¢, {change.get('deletions', 0)} è¡Œåˆ é™¤",
                    'timestamp': latest_analysis['created_at'],
                    'codeSnippet': change.get('patch', '')[:500] if change.get('patch') else '',
                    'fileDiff': {
                        'oldContent': '',
                        'newContent': change.get('patch', '')
                    },
                    'testCase': {
                        'name': f"Test for {change.get('file', 'file')}",
                        'code': f"// Test case for {change.get('file', 'file')}\n// TODO: Implement test"
                    }
                })
        
        return jsonify(analysis_response)
        
    except Exception as e:
        logger.error(f"è·å–é¡¹ç›®åˆ†æç»“æœå¤±è´¥: {str(e)}")
        return jsonify({
            'status': 'error',
            'message': str(e),
            'project_id': id
        }), 500

# è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆé»˜è®¤æµ‹è¯•ç”¨ä¾‹

def _generate_smart_default_test(analysis_data: dict) -> str:
    """ç”Ÿæˆæ™ºèƒ½çš„é»˜è®¤å›å½’æµ‹è¯•ä»£ç """
    changes = []
    if 'analysis_data' in analysis_data and 'result' in analysis_data['analysis_data']:
        changes = analysis_data['analysis_data']['result'].get('changes', [])
    
    test_code = f"""
// ä»£ç å˜æ›´å›å½’æµ‹è¯•
// åŸºäº {len(changes)} ä¸ªæ–‡ä»¶å˜æ›´ç”Ÿæˆçš„æ™ºèƒ½æµ‹è¯•

describe('ä»£ç å˜æ›´å›å½’æµ‹è¯•', () => {{
    let testContext = {{}};
    
    beforeAll(async () => {{
        // åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ
        testContext.apiBase = process.env.API_BASE_URL || 'http://localhost:5000';
        testContext.timeout = 30000;
    }});
    
    describe('æ ¸å¿ƒAPIåŠŸèƒ½éªŒè¯', () => {{
        test('éªŒè¯é¡¹ç›®åˆ—è¡¨API', async () => {{
            const response = await fetch(`${{testContext.apiBase}}/api/projects`);
            expect(response.status).toBe(200);
            
            const projects = await response.json();
            expect(Array.isArray(projects)).toBe(true);
        }});
        
        test('éªŒè¯é¡¹ç›®è¯¦æƒ…API', async () => {{
            const response = await fetch(`${{testContext.apiBase}}/api/projects/1`);
            expect(response.status).toBeOneOf([200, 404]);
            
            if (response.status === 200) {{
                const project = await response.json();
                expect(project).toHaveProperty('project');
            }}
        }});
    }});
    
    describe('æ•°æ®å®Œæ•´æ€§æ£€æŸ¥', () => {{"""
    
    # æ ¹æ®å˜æ›´çš„æ–‡ä»¶ç±»å‹æ·»åŠ ç‰¹å®šæµ‹è¯•
    for change in changes:
        file_path = change.get('file', '')
        if 'api' in file_path.lower() or 'route' in file_path.lower():
            test_code += f"""
        
        test('éªŒè¯ {os.path.basename(file_path)} APIå˜æ›´', async () => {{
            // æµ‹è¯•å˜æ›´åçš„APIåŠŸèƒ½
            const testData = {{ test: true }};
            
            try {{
                const response = await fetch(`${{testContext.apiBase}}/api/test-endpoint`, {{
                    method: 'POST',
                    headers: {{ 'Content-Type': 'application/json' }},
                    body: JSON.stringify(testData)
                }});
                
                // éªŒè¯å“åº”æ ¼å¼å’ŒçŠ¶æ€
                expect([200, 201, 400, 404]).toContain(response.status);
                
                if (response.headers.get('content-type')?.includes('application/json')) {{
                    const data = await response.json();
                    expect(data).toBeDefined();
                }}
            }} catch (error) {{
                // ç½‘ç»œé”™è¯¯æˆ–æœåŠ¡ä¸å¯ç”¨æ˜¯å¯æ¥å—çš„
                expect(error.message).toContain('fetch');
            }}
        }});"""
    
    test_code += f"""
    }});
    
    describe('é”™è¯¯å¤„ç†éªŒè¯', () => {{
        test('éªŒè¯æ— æ•ˆè¯·æ±‚å¤„ç†', async () => {{
            const response = await fetch(`${{testContext.apiBase}}/api/nonexistent`);
            expect(response.status).toBe(404);
        }});
        
        test('éªŒè¯æ— æ•ˆæ•°æ®å¤„ç†', async () => {{
            const response = await fetch(`${{testContext.apiBase}}/api/projects`, {{
                method: 'POST',
                headers: {{ 'Content-Type': 'application/json' }},
                body: JSON.stringify({{ invalid: 'data' }})
            }});
            
            expect([400, 422, 500]).toContain(response.status);
        }});
    }});
    
    afterAll(() => {{
        // æ¸…ç†æµ‹è¯•ç¯å¢ƒ
        console.log('å›å½’æµ‹è¯•å®Œæˆ');
    }});
}});

/*
æµ‹è¯•è¦†ç›–èŒƒå›´ï¼š
{chr(10).join([f"- {change.get('file', 'Unknown')}: {change.get('type', 'modified')}" for change in changes])}

å»ºè®®çš„æ‰‹åŠ¨éªŒè¯ç‚¹ï¼š
1. æ£€æŸ¥æ ¸å¿ƒä¸šåŠ¡æµç¨‹æ˜¯å¦æ­£å¸¸
2. éªŒè¯ç”¨æˆ·ç•Œé¢å“åº”æ€§
3. ç¡®è®¤æ•°æ®æŒä¹…åŒ–æ­£ç¡®æ€§
4. æµ‹è¯•å¼‚å¸¸åœºæ™¯å¤„ç†
5. éªŒè¯æ€§èƒ½æŒ‡æ ‡æœªé™çº§

é£é™©è¯„ä¼°ï¼šåŸºäº {len(changes)} ä¸ªæ–‡ä»¶å˜æ›´ï¼Œå»ºè®®æ‰§è¡Œå®Œæ•´å›å½’æµ‹è¯•
*/"""

def _generate_e2e_default_test(analysis_data: dict) -> str:
    """ç”Ÿæˆç«¯åˆ°ç«¯é»˜è®¤æµ‹è¯•ä»£ç """
    return """
// ç«¯åˆ°ç«¯åŠŸèƒ½éªŒè¯æµ‹è¯•
describe('E2E åŠŸèƒ½å®Œæ•´æ€§éªŒè¯', () => {
    let page;
    
    beforeAll(async () => {
        // å¯åŠ¨æµè§ˆå™¨å’Œé¡µé¢
        page = await browser.newPage();
        await page.goto('http://localhost:5173');
    });
    
    describe('ç”¨æˆ·ç•Œé¢åŸºç¡€åŠŸèƒ½', () => {
        test('ä¸»é¡µé¢æ­£å¸¸åŠ è½½', async () => {
            await page.waitForSelector('body', { timeout: 30000 });
            
            const title = await page.title();
            expect(title).toBeTruthy();
            
            // æ£€æŸ¥é¡µé¢åŸºæœ¬å…ƒç´ 
            const mainContent = await page.$('.main-content, #app, main');
            expect(mainContent).toBeTruthy();
        });
        
        test('å¯¼èˆªåŠŸèƒ½æ­£å¸¸', async () => {
            // æŸ¥æ‰¾å¯¼èˆªé“¾æ¥
            const navLinks = await page.$$('nav a, .nav-link, [role="navigation"] a');
            
            if (navLinks.length > 0) {
                // ç‚¹å‡»ç¬¬ä¸€ä¸ªå¯¼èˆªé“¾æ¥
                await navLinks[0].click();
                await page.waitForTimeout(1000);
                
                // éªŒè¯é¡µé¢å˜åŒ–
                const currentUrl = page.url();
                expect(currentUrl).toBeTruthy();
            }
        });
    });
    
    describe('æ ¸å¿ƒä¸šåŠ¡æµç¨‹', () => {
        test('é¡¹ç›®åˆ—è¡¨æŸ¥çœ‹', async () => {
            // å°è¯•å¯¼èˆªåˆ°é¡¹ç›®åˆ—è¡¨
            try {
                await page.goto('http://localhost:5173/projects');
                await page.waitForSelector('body', { timeout: 10000 });
                
                // æ£€æŸ¥æ˜¯å¦æœ‰é¡¹ç›®åˆ—è¡¨æˆ–ç›¸å…³å†…å®¹
                const hasContent = await page.evaluate(() => {
                    return document.body.textContent.trim().length > 0;
                });
                
                expect(hasContent).toBe(true);
            } catch (error) {
                console.log('é¡¹ç›®åˆ—è¡¨é¡µé¢ä¸å¯è®¿é—®æˆ–ä¸å­˜åœ¨');
            }
        });
        
        test('ç”¨æˆ·äº¤äº’å“åº”', async () => {
            // æŸ¥æ‰¾å¯ç‚¹å‡»å…ƒç´ 
            const clickableElements = await page.$$('button, a, [role="button"]');
            
            if (clickableElements.length > 0) {
                const initialUrl = page.url();
                
                // ç‚¹å‡»ç¬¬ä¸€ä¸ªå¯ç‚¹å‡»å…ƒç´ 
                await clickableElements[0].click();
                await page.waitForTimeout(1000);
                
                // éªŒè¯æœ‰å“åº”ï¼ˆURLå˜åŒ–æˆ–é¡µé¢å†…å®¹å˜åŒ–ï¼‰
                const newUrl = page.url();
                const responseDetected = newUrl !== initialUrl || 
                    await page.$('.modal, .dialog, .popup, .notification');
                
                expect(typeof responseDetected).toBe('boolean');
            }
        });
    });
    
    describe('é”™è¯¯åœºæ™¯å¤„ç†', () => {
        test('404é¡µé¢å¤„ç†', async () => {
            await page.goto('http://localhost:5173/nonexistent-page');
            await page.waitForTimeout(2000);
            
            // æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰é”™è¯¯å¤„ç†
            const pageContent = await page.content();
            const hasErrorHandling = pageContent.includes('404') || 
                                   pageContent.includes('Not Found') || 
                                   pageContent.includes('é¡µé¢ä¸å­˜åœ¨');
            
            // 404å¤„ç†å­˜åœ¨æˆ–é¡µé¢é‡å®šå‘éƒ½æ˜¯å¯æ¥å—çš„
            expect(typeof hasErrorHandling).toBe('boolean');
        });
    });
    
    afterAll(async () => {
        if (page) {
            await page.close();
        }
    });
});

/*
E2Eæµ‹è¯•è¯´æ˜ï¼š
- éªŒè¯å‰ç«¯åº”ç”¨åŸºæœ¬åŠŸèƒ½
- æµ‹è¯•ç”¨æˆ·äº¤äº’æµç¨‹
- æ£€æŸ¥é”™è¯¯åœºæ™¯å¤„ç†
- ç¡®ä¿ç•Œé¢å“åº”æ­£å¸¸

æ³¨æ„ï¼šæ­¤æµ‹è¯•è®¾è®¡ä¸ºå…¼å®¹æ€§æµ‹è¯•ï¼Œå³ä½¿æŸäº›åŠŸèƒ½ä¸å­˜åœ¨ä¹Ÿä¸ä¼šå¤±è´¥
å®é™…ä½¿ç”¨æ—¶åº”æ ¹æ®å…·ä½“ä¸šåŠ¡é€»è¾‘è°ƒæ•´æµ‹è¯•ç”¨ä¾‹
*/"""

@api.route('/projects/<int:id>/code-diff', methods=['GET'])
def get_code_diff(id):
    """è·å–é¡¹ç›®çš„ä»£ç å·®å¼‚å’Œå½±å“åˆ†æ"""
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        commit_hash = request.args.get('commit')
        since_commit = request.args.get('since')
        branch = request.args.get('branch', 'main')
        
        # è·å–é¡¹ç›®ä¿¡æ¯
        project = db.fetch_one("SELECT * FROM projects WHERE id = %s", (id,))
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        # å…‹éš†æˆ–æ›´æ–°ä»“åº“
        git_utils = GitUtils(project['git_url'], branch)
        
        if not git_utils.clone_repo():
            return jsonify({'error': 'æ— æ³•å…‹éš†ä»“åº“'}), 500
        
        try:
            # ä½¿ç”¨å¢å¼ºåˆ†æå™¨åˆ†æä»£ç å˜æ›´
            analyzer = EnhancedCursorAnalyzer(git_utils.repo_path)
            
            # å¼‚æ­¥è¿è¡Œåˆ†æ
            @run_async
            async def analyze_changes():
                return await analyzer.analyze_repository_changes(commit_hash)
            
            analysis_result = analyze_changes()
            
            # å¦‚æœåˆ†ææˆåŠŸï¼Œæ ¼å¼åŒ–è¿”å›æ•°æ®
            if hasattr(analysis_result, 'status') and analysis_result.status == 'success':
                diff_data = {
                    'status': 'success',
                    'commit_hash': commit_hash,
                    'changes_count': analysis_result.changes_count,
                    'analysis_summary': {
                        'total_changes': analysis_result.changes_count,
                        'high_risk_changes': len([r for r in analysis_result.analysis_results 
                                                if hasattr(r.change, 'risk_level') and r.change.risk_level == 'high']),
                        'affected_files': len(set([r.change.file_path for r in analysis_result.analysis_results])),
                        'recommended_tests': sum([len(r.impact.test_cases) for r in analysis_result.analysis_results])
                    },
                    'changes': []
                }
                
                # å¤„ç†æ¯ä¸ªå˜æ›´
                for result in analysis_result.analysis_results:
                    change_data = {
                        'file_path': result.change.file_path,
                        'change_type': result.change.change_type,
                        'risk_level': getattr(result.change, 'risk_level', 'medium'),
                        'affected_functions': getattr(result.change, 'affected_functions', []),
                        'affected_classes': getattr(result.change, 'affected_classes', []),
                        'complexity_delta': getattr(result.change, 'complexity_delta', 8),
                        'business_impact': getattr(result.change, 'business_impact', []),
                        'impact_analysis': {
                            'direct_impacts': result.impact.direct_impacts if hasattr(result.impact, 'direct_impacts') else [],
                            'indirect_impacts': result.impact.indirect_impacts if hasattr(result.impact, 'indirect_impacts') else [],
                            'risk_factors': result.impact.risk_factors if hasattr(result.impact, 'risk_factors') else [],
                            'confidence': result.impact.confidence if hasattr(result.impact, 'confidence') else 0.8
                        },
                        'test_cases': [
                            {
                                'id': tc.test_id if hasattr(tc, 'test_id') else f"test_{i}",
                                'name': tc.name if hasattr(tc, 'name') else f"Test for {result.change.file_path}",
                                'type': tc.test_type if hasattr(tc, 'test_type') else 'unit',
                                'priority': tc.priority if hasattr(tc, 'priority') else 'medium',
                                'description': tc.description if hasattr(tc, 'description') else f"Test case for {result.change.change_type} in {result.change.file_path}",
                                'test_code': tc.test_code if hasattr(tc, 'test_code') else generate_default_test_code(result.change),
                                'estimated_time': tc.estimated_time if hasattr(tc, 'estimated_time') else 10,
                                'coverage_areas': tc.coverage_areas if hasattr(tc, 'coverage_areas') else []
                            }
                            for i, tc in enumerate(result.impact.test_cases if hasattr(result.impact, 'test_cases') else [])
                        ]
                    }
                    
                    # å¦‚æœæ²¡æœ‰ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹ï¼Œåˆ›å»ºé»˜è®¤æµ‹è¯•ç”¨ä¾‹
                    if not change_data['test_cases']:
                        change_data['test_cases'] = [create_default_test_case(result.change)]
                    
                    diff_data['changes'].append(change_data)
                
                return jsonify(diff_data)
            
            else:
                # åˆ†æå¤±è´¥ï¼Œè¿”å›åŸºç¡€gitå·®å¼‚
                return get_basic_git_diff(git_utils, commit_hash, since_commit)
                
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            git_utils.cleanup()
            
    except Exception as e:
        logger.error(f"è·å–ä»£ç å·®å¼‚å¤±è´¥: {str(e)}")
        return jsonify({'error': f'åˆ†æå¤±è´¥: {str(e)}'}), 500

def get_basic_git_diff(git_utils, commit_hash=None, since_commit=None):
    """è·å–åŸºç¡€gitå·®å¼‚åˆ†æï¼ˆå½“AIåˆ†æå¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
    try:
        logger.info("å¼€å§‹åŸºç¡€gitå·®å¼‚åˆ†æ")
        
        if not git_utils.repo:
            raise Exception("Gitä»“åº“æœªåˆå§‹åŒ–")
        
        commits = list(git_utils.repo.iter_commits(max_count=5))
        if not commits:
            raise Exception("ä»“åº“ä¸­æ²¡æœ‰æäº¤")
        
        target_commit = commits[0]  # æœ€æ–°æäº¤
        if commit_hash and commit_hash != 'latest':
            try:
                target_commit = git_utils.repo.commit(commit_hash)
            except:
                logger.warning(f"æ‰¾ä¸åˆ°æŒ‡å®šæäº¤ {commit_hash}ï¼Œä½¿ç”¨æœ€æ–°æäº¤")
        
        if not target_commit.parents:
            # åˆå§‹æäº¤ï¼Œè¿”å›ç®€å•åˆ†æ
            return {
                'status': 'success',
                'commit_hash': target_commit.hexsha,
                'changes_count': 1,
                'analysis_summary': {
                    'total_changes': 1,
                    'high_risk_changes': 0,
                    'affected_files': 1,
                    'recommended_tests': 1
                },
                'changes': [{
                    'file_path': 'åˆå§‹é¡¹ç›®æ–‡ä»¶',
                    'change_type': 'initial',
                    'risk_level': 'low',
                    'affected_functions': [],
                    'affected_classes': [],
                    'complexity_delta': 3,
                    'business_impact': ['é¡¹ç›®åˆå§‹åŒ–'],
                    'impact_analysis': {
                        'direct_impacts': ['é¡¹ç›®ç»“æ„'],
                        'indirect_impacts': [],
                        'risk_factors': [],
                        'confidence': 0.8
                    },
                    'test_cases': [_create_simple_test_case('project_init', 'initial')]
                }]
            }
        
        # è·å–å·®å¼‚
        parent_commit = target_commit.parents[0]
        diffs = parent_commit.diff(target_commit)
        
        changes = []
        for diff in diffs:
            file_path = diff.a_path or diff.b_path
            
            # åªå¤„ç†ä»£ç æ–‡ä»¶
            if not _is_code_file_simple(file_path):
                continue
            
            change_data = {
                'file_path': file_path,
                'change_type': _get_change_type(diff),
                'risk_level': _assess_simple_risk(file_path, diff),
                'affected_functions': [],
                'affected_classes': [],
                'complexity_delta': _calculate_complexity_for_diff(diff, file_path),
                'business_impact': _get_simple_business_impact(file_path),
                'impact_analysis': {
                    'direct_impacts': [file_path],
                    'indirect_impacts': [],
                    'risk_factors': [],
                    'confidence': 0.6
                },
                'test_cases': [_create_simple_test_case(file_path, _get_change_type(diff))]
            }
            changes.append(change_data)
        
        return {
            'status': 'success',
            'commit_hash': commit_hash,
            'changes_count': len(changes),
            'analysis_summary': {
                'total_changes': len(changes),
                'high_risk_changes': len([c for c in changes if c['risk_level'] == 'high']),
                'affected_files': len(changes),
                'recommended_tests': len(changes)
            },
            'changes': changes
        }
        
    except Exception as e:
        logger.error(f"è·å–åŸºç¡€gitå·®å¼‚å¤±è´¥: {str(e)}")
        return {'error': f'Gitå·®å¼‚åˆ†æå¤±è´¥: {str(e)}'}

def _calculate_complexity_for_diff(diff, file_path):
    """è®¡ç®—diffçš„å¤æ‚åº¦"""
    try:
        # åŸºç¡€å¤æ‚åº¦è®¡ç®—
        insertions = 0
        deletions = 0
        
        # å°è¯•è·å–å®é™…çš„æ’å…¥/åˆ é™¤è¡Œæ•°
        try:
            if diff.a_blob and diff.b_blob:
                # ä¿®æ”¹çš„æ–‡ä»¶
                a_content = diff.a_blob.data_stream.read().decode('utf-8', errors='ignore')
                b_content = diff.b_blob.data_stream.read().decode('utf-8', errors='ignore')
                a_lines = len(a_content.split('\n'))
                b_lines = len(b_content.split('\n'))
                
                if b_lines > a_lines:
                    insertions = b_lines - a_lines
                elif a_lines > b_lines:
                    deletions = a_lines - b_lines
                else:
                    insertions = max(1, b_lines // 20)  # ä¼°ç®—æœ‰å˜æ›´
                    
            elif diff.new_file and diff.b_blob:
                # æ–°æ–‡ä»¶
                content = diff.b_blob.data_stream.read().decode('utf-8', errors='ignore')
                insertions = len(content.split('\n'))
                
            elif diff.deleted_file and diff.a_blob:
                # åˆ é™¤çš„æ–‡ä»¶
                content = diff.a_blob.data_stream.read().decode('utf-8', errors='ignore')
                deletions = len(content.split('\n'))
                
        except Exception as e:
            logger.warning(f"è®¡ç®—è¡Œæ•°å¤±è´¥: {str(e)}")
            # ä½¿ç”¨ä¼°ç®—å€¼
            if diff.new_file:
                insertions = 50
            elif diff.deleted_file:
                deletions = 30
            else:
                insertions = 10
                deletions = 5
        
        # è®¡ç®—å¤æ‚åº¦
        lines_changed = insertions + deletions
        if lines_changed == 0:
            base_complexity = 2
        elif lines_changed < 10:
            base_complexity = 3
        elif lines_changed < 50:
            base_complexity = 8
        elif lines_changed < 100:
            base_complexity = 15
        else:
            base_complexity = max(15, lines_changed // 8)
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹è°ƒæ•´
        file_ext = os.path.splitext(file_path)[1].lower()
        if file_ext in ['.py', '.java', '.cpp', '.ts']:
            base_complexity = int(base_complexity * 1.2)
        elif file_ext in ['.js', '.vue']:
            base_complexity = int(base_complexity * 1.1)
        
        # æ ¹æ®å˜æ›´ç±»å‹è°ƒæ•´
        if diff.new_file:
            base_complexity = int(base_complexity * 1.5)
        elif diff.deleted_file:
            base_complexity = int(base_complexity * 0.7)
        
        return max(2, min(base_complexity, 100))
        
    except Exception as e:
        logger.warning(f"å¤æ‚åº¦è®¡ç®—å¤±è´¥: {str(e)}")
        return 5  # é»˜è®¤å¤æ‚åº¦

def _is_code_file_simple(file_path):
    """ç®€å•çš„ä»£ç æ–‡ä»¶åˆ¤æ–­"""
    if not file_path:
        return False
    
    code_extensions = {'.py', '.js', '.vue', '.ts', '.tsx', '.jsx', '.java', '.cpp', '.c', '.h', '.cs', '.php', '.rb', '.go'}
    ext = os.path.splitext(file_path)[1].lower()
    return ext in code_extensions

def _get_change_type(diff):
    """è·å–å˜æ›´ç±»å‹"""
    if diff.new_file:
        return 'added'
    elif diff.deleted_file:
        return 'deleted'
    else:
        return 'modified'

def _assess_simple_risk(file_path, diff):
    """ç®€å•çš„é£é™©è¯„ä¼°"""
    # æ ¹æ®æ–‡ä»¶è·¯å¾„å’Œå˜æ›´ç±»å‹è¯„ä¼°é£é™©
    if 'api' in file_path.lower() or 'route' in file_path.lower():
        return 'high'
    elif 'test' in file_path.lower():
        return 'low'
    elif diff.new_file or diff.deleted_file:
        return 'medium'
    else:
        return 'medium'

def _get_simple_business_impact(file_path):
    """ç®€å•çš„ä¸šåŠ¡å½±å“åˆ†æ"""
    impacts = []
    
    if 'api' in file_path.lower():
        impacts.append('APIæ¥å£å˜æ›´')
    if 'user' in file_path.lower():
        impacts.append('ç”¨æˆ·åŠŸèƒ½å½±å“')
    if 'auth' in file_path.lower():
        impacts.append('è®¤è¯æˆæƒå˜æ›´')
    if 'database' in file_path.lower() or 'model' in file_path.lower():
        impacts.append('æ•°æ®å±‚å˜æ›´')
    
    return impacts if impacts else ['åŠŸèƒ½å˜æ›´']

def _create_simple_test_case(file_path, change_type):
    """åˆ›å»ºç®€å•çš„æµ‹è¯•ç”¨ä¾‹"""
    return {
        'id': f"test_{abs(hash(file_path)) % 10000}",
        'name': f"æµ‹è¯• {os.path.basename(file_path)} {change_type}å˜æ›´",
        'type': 'unit' if file_path.endswith('.py') else 'integration',
        'priority': 'high' if 'api' in file_path.lower() else 'medium',
        'description': f"éªŒè¯{file_path}åœ¨{change_type}åçš„åŠŸèƒ½æ­£ç¡®æ€§",
        'test_code': generate_default_test_code_for_file(file_path, change_type),
        'estimated_time': 15,
        'coverage_areas': ['åŠŸèƒ½éªŒè¯', 'å›å½’æµ‹è¯•']
    }

def generate_default_test_code_for_file(file_path, change_type):
    """ä¸ºæ–‡ä»¶ç”Ÿæˆé»˜è®¤æµ‹è¯•ä»£ç """
    file_name = os.path.basename(file_path)
    file_ext = os.path.splitext(file_path)[1]
    
    if file_ext == '.py':
        return f'''# Pythonæµ‹è¯•ä»£ç  - {file_name}
import unittest
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

class Test{file_name.replace('.py', '').title()}(unittest.TestCase):
    """æµ‹è¯•{file_name}çš„{change_type}å˜æ›´"""
    
    def setUp(self):
        """æµ‹è¯•å‰å‡†å¤‡"""
        pass
    
    def test_{change_type}_functionality(self):
        """æµ‹è¯•{change_type}å˜æ›´åçš„åŠŸèƒ½"""
        # TODO: æ·»åŠ å…·ä½“çš„æµ‹è¯•é€»è¾‘
        self.assertTrue(True, "åŸºç¡€æµ‹è¯•é€šè¿‡")
    
    def test_regression_check(self):
        """å›å½’æµ‹è¯•"""
        # TODO: éªŒè¯ç°æœ‰åŠŸèƒ½æœªå—å½±å“
        self.assertTrue(True, "å›å½’æµ‹è¯•é€šè¿‡")
    
    def tearDown(self):
        """æµ‹è¯•åæ¸…ç†"""
        pass

if __name__ == '__main__':
    unittest.main()
'''
    elif file_ext in ['.js', '.vue', '.ts']:
        return f'''// JavaScript/Vueæµ‹è¯•ä»£ç  - {file_name}
describe('{file_name} {change_type}å˜æ›´æµ‹è¯•', () => {{
  beforeEach(() => {{
    // æµ‹è¯•å‰å‡†å¤‡
  }});
  
  test('éªŒè¯{change_type}å˜æ›´åŠŸèƒ½', () => {{
    // TODO: æ·»åŠ å…·ä½“çš„æµ‹è¯•é€»è¾‘
    expect(true).toBe(true);
  }});
  
  test('å›å½’æµ‹è¯•éªŒè¯', () => {{
    // TODO: éªŒè¯ç°æœ‰åŠŸèƒ½æœªå—å½±å“
    expect(true).toBe(true);
  }});
  
  afterEach(() => {{
    // æµ‹è¯•åæ¸…ç†
  }});
}});
'''
    else:
        return f'''# é€šç”¨æµ‹è¯•ä»£ç  - {file_name}
# æµ‹è¯•{change_type}å˜æ›´

## æµ‹è¯•è®¡åˆ’
1. åŠŸèƒ½éªŒè¯æµ‹è¯•
2. å›å½’æµ‹è¯•
3. é›†æˆæµ‹è¯•

## æµ‹è¯•ç”¨ä¾‹
- [ ] éªŒè¯{change_type}å˜æ›´åçš„æ ¸å¿ƒåŠŸèƒ½
- [ ] ç¡®ä¿ç°æœ‰åŠŸèƒ½ä¸å—å½±å“
- [ ] éªŒè¯ä¸å…¶ä»–æ¨¡å—çš„é›†æˆ

## é¢„æœŸç»“æœ
æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹é€šè¿‡ï¼ŒåŠŸèƒ½æ­£å¸¸è¿è¡Œ
'''

def create_default_test_case(change):
    """åˆ›å»ºé»˜è®¤æµ‹è¯•ç”¨ä¾‹"""
    return {
        'id': f"test_{abs(hash(change.file_path)) % 10000}",
        'name': f"æµ‹è¯• {os.path.basename(change.file_path)} {change.change_type}å˜æ›´",
        'type': 'unit',
        'priority': 'medium',
        'description': f"éªŒè¯{change.file_path}åœ¨{change.change_type}åçš„åŠŸèƒ½æ­£ç¡®æ€§",
        'test_code': generate_default_test_code(change),
        'estimated_time': 10,
        'coverage_areas': ['åŠŸèƒ½éªŒè¯']
    }

def generate_default_test_code(change):
    """ç”Ÿæˆé»˜è®¤æµ‹è¯•ä»£ç """
    return f'''# é»˜è®¤æµ‹è¯•ä»£ç 
# æ–‡ä»¶: {change.file_path}
# å˜æ›´ç±»å‹: {change.change_type}

def test_{change.change_type}_functionality():
    """æµ‹è¯•{change.change_type}å˜æ›´åçš„åŠŸèƒ½"""
    # TODO: å®ç°å…·ä½“æµ‹è¯•é€»è¾‘
    assert True, "åŸºç¡€æµ‹è¯•é€šè¿‡"

def test_regression():
    """å›å½’æµ‹è¯•"""  
    # TODO: éªŒè¯ç°æœ‰åŠŸèƒ½æœªå—å½±å“
    assert True, "å›å½’æµ‹è¯•é€šè¿‡"
'''

@api.route('/projects/<int:id>/commits', methods=['GET'])
def get_project_commits(id):
    """è·å–é¡¹ç›®çš„å†å²æäº¤åˆ—è¡¨"""
    try:
        # è·å–é¡¹ç›®ä¿¡æ¯
        project = db.fetch_one("SELECT * FROM projects WHERE id = %s", (id,))
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        # è·å–æŸ¥è¯¢å‚æ•°
        limit = int(request.args.get('limit', 20))
        branch = request.args.get('branch', project.get('branch', 'main'))
        
        # å…‹éš†ä»“åº“
        git_utils = GitUtils(project['git_url'], branch)
        
        if not git_utils.clone_repo():
            return jsonify({'error': 'æ— æ³•å…‹éš†ä»“åº“'}), 500
        
        try:
            if not git_utils.repo:
                return jsonify({'error': 'ä»“åº“æœªåˆå§‹åŒ–'}), 500
            
            # è·å–å†å²æäº¤
            commits = []
            for commit in git_utils.repo.iter_commits(max_count=limit):
                commit_info = {
                    'hash': commit.hexsha,
                    'short_hash': commit.hexsha[:8],
                    'message': commit.message.strip(),
                    'author': commit.author.name,
                    'author_email': commit.author.email,
                    'date': commit.committed_datetime.isoformat(),
                    'parents': [p.hexsha for p in commit.parents],
                    'stats': None
                }
                
                # è·å–æäº¤ç»Ÿè®¡ä¿¡æ¯
                try:
                    if commit.parents:
                        parent = commit.parents[0]
                        diffs = parent.diff(commit)
                        
                        files_changed = len(diffs)
                        insertions = 0
                        deletions = 0
                        
                        for diff in diffs:
                            try:
                                # ç®€åŒ–ç»Ÿè®¡è®¡ç®—ï¼Œé¿å…å¤æ‚é”™è¯¯
                                if diff.new_file:
                                    insertions += 50  # ä¼°ç®—å€¼
                                elif diff.deleted_file:
                                    deletions += 50  # ä¼°ç®—å€¼
                                else:
                                    insertions += 20  # ä¼°ç®—å€¼
                                    deletions += 10   # ä¼°ç®—å€¼
                            except:
                                continue
                        
                        commit_info['stats'] = {
                            'files_changed': files_changed,
                            'insertions': insertions,
                            'deletions': deletions
                        }
                except Exception as e:
                    logger.warning(f"è·å–æäº¤ç»Ÿè®¡å¤±è´¥: {str(e)}")
                    commit_info['stats'] = {
                        'files_changed': 0,
                        'insertions': 0,
                        'deletions': 0
                    }
                
                commits.append(commit_info)
            
            return jsonify({
                'status': 'success',
                'commits': commits,
                'total': len(commits),
                'branch': branch
            })
            
        finally:
            git_utils.cleanup()
            
    except Exception as e:
        logger.error(f"è·å–å†å²æäº¤å¤±è´¥: {str(e)}")
        return jsonify({'error': f'è·å–å†å²æäº¤å¤±è´¥: {str(e)}'}), 500

# åœ¨ä»£ç å·®å¼‚åˆ†æAPIä¸­æ·»åŠ æ•°æ®åº“å­˜å‚¨
@api.route('/projects/<int:id>/code-diff/save', methods=['POST'])
def save_code_diff_analysis(id):
    """ä¿å­˜ä»£ç å·®å¼‚åˆ†æç»“æœåˆ°æ•°æ®åº“"""
    try:
        data = request.json
        commit_hash = data.get('commit_hash')
        analysis_result = data.get('analysis_result')
        
        if not commit_hash or not analysis_result:
            return jsonify({'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        result = db.execute(
            """INSERT INTO analysis_results 
               (project_id, commit_hash, analysis_type, result_data, created_at) 
               VALUES (%s, %s, %s, %s, %s) RETURNING id""",
            (id, commit_hash, 'code_diff', json.dumps(analysis_result), datetime.now())
        )
        
        if result and 'id' in result:
            return jsonify({'id': result['id'], 'message': 'åˆ†æç»“æœå·²ä¿å­˜'}), 201
        else:
            return jsonify({'error': 'ä¿å­˜å¤±è´¥'}), 500
            
    except Exception as e:
        logger.error(f"ä¿å­˜ä»£ç å·®å¼‚åˆ†æå¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

@api.route('/projects/<int:id>/code-diff/history', methods=['GET'])
def get_code_diff_history(id):
    """è·å–é¡¹ç›®çš„å†å²å·®å¼‚åˆ†æç»“æœ"""
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        limit = int(request.args.get('limit', 10))
        
        # ä»æ•°æ®åº“è·å–å†å²åˆ†æç»“æœ
        results = db.fetch_all(
            """SELECT * FROM analysis_results 
               WHERE project_id = %s AND analysis_type = 'code_diff' 
               ORDER BY created_at DESC LIMIT %s""",
            (id, limit)
        )
        
        # å¤„ç†ç»“æœæ•°æ®
        history = []
        for result in results:
            try:
                result_data = json.loads(result['result_data']) if isinstance(result['result_data'], str) else result['result_data']
                history.append({
                    'id': result['id'],
                    'commit_hash': result['commit_hash'],
                    'created_at': result['created_at'],
                    'analysis_summary': result_data.get('analysis_summary', {}),
                    'changes_count': result_data.get('changes_count', 0)
                })
            except Exception as e:
                logger.warning(f"è§£æåˆ†æç»“æœå¤±è´¥: {str(e)}")
                continue
        
        return jsonify({
            'status': 'success',
            'history': history,
            'total': len(history)
        })
        
    except Exception as e:
        logger.error(f"è·å–å†å²åˆ†æå¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

# åœ¨æ–‡ä»¶æœ«å°¾æ·»åŠ æ™ºèƒ½æµ‹è¯•ç”ŸæˆAPI

@api.route('/projects/<int:id>/intelligent-tests', methods=['POST'])
def generate_intelligent_tests(id):
    """ç”ŸæˆåŸºäºé¡¹ç›®æ•´ä½“åˆ†æçš„æ™ºèƒ½æµ‹è¯•ç”¨ä¾‹"""
    try:
        # è·å–é¡¹ç›®ä¿¡æ¯
        project = db.fetch_one("SELECT * FROM projects WHERE id = %s", (id,))
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        # è·å–è¯·æ±‚å‚æ•°
        data = request.json or {}
        changed_files = data.get('changed_files', [])
        change_type = data.get('change_type', 'modified')
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå˜æ›´æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤çš„æ–‡ä»¶åˆ—è¡¨
        if not changed_files:
            # ä¸ä¾èµ–git_utils.get_recent_changesï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥
            changed_files = []  # ç©ºåˆ—è¡¨å°†è®©æ™ºèƒ½ç”Ÿæˆå™¨åˆ†ææ•´ä¸ªé¡¹ç›®ç»“æ„
        
        # åˆ›å»ºæ™ºèƒ½æµ‹è¯•ç”Ÿæˆå™¨
        from generators.intelligent_test_generator import IntelligentTestGenerator
        
        # ä½¿ç”¨ä¸´æ—¶ç›®å½•è¿›è¡Œé¡¹ç›®åˆ†æ
        git_utils = GitUtils(project['git_url'], project.get('branch', 'main'))
        if not git_utils.clone_repo():
            return jsonify({'error': 'æ— æ³•å…‹éš†ä»“åº“è¿›è¡Œåˆ†æ'}), 500
        
        try:
            generator = IntelligentTestGenerator(git_utils.repo_path)
            
            # åˆ†æé¡¹ç›®ç»“æ„
            if not generator.analyze_project_structure():
                return jsonify({'error': 'é¡¹ç›®ç»“æ„åˆ†æå¤±è´¥'}), 500
            
            # ç”Ÿæˆæ™ºèƒ½æµ‹è¯•ç”¨ä¾‹
            test_cases = generator.generate_intelligent_tests(changed_files, change_type)
            
            # ä¿å­˜ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹åˆ°æ•°æ®åº“
            saved_tests = []
            for test_case in test_cases:
                result = db.execute(
                    """INSERT INTO test_cases 
                       (project_id, name, description, test_type, priority, test_code, 
                        estimated_time, coverage_areas, file_path, function_name, created_at) 
                       VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s) RETURNING id""",
                    (
                        id, 
                        test_case['name'], 
                        test_case['description'], 
                        test_case['type'], 
                        test_case['priority'],
                        test_case['test_code'], 
                        test_case['estimated_time'],
                        json.dumps(test_case.get('coverage_areas', [])),
                        test_case.get('file_path', ''),
                        test_case.get('function_name', ''),
                        datetime.now()
                    )
                )
                
                if result and 'id' in result:
                    test_case['id'] = result['id']
                    saved_tests.append(test_case)
            
            return jsonify({
                'status': 'success',
                'message': f'æˆåŠŸç”Ÿæˆ {len(saved_tests)} ä¸ªæ™ºèƒ½æµ‹è¯•ç”¨ä¾‹',
                'test_cases': saved_tests,
                'project_analysis': {
                    'total_files': len(generator.project_structure.get('python_files', [])) + 
                                 len(generator.project_structure.get('js_files', [])) + 
                                 len(generator.project_structure.get('vue_files', [])),
                    'api_files': len(generator.project_structure.get('api_files', [])),
                    'core_functions': len(generator.core_functions)
                }
            })
            
        finally:
            git_utils.cleanup()
            
    except Exception as e:
        logger.error(f"æ™ºèƒ½æµ‹è¯•ç”Ÿæˆå¤±è´¥: {str(e)}")
        return jsonify({'error': f'æ™ºèƒ½æµ‹è¯•ç”Ÿæˆå¤±è´¥: {str(e)}'}), 500

@api.route('/projects/<int:project_id>/generate-tests', methods=['POST'])
def generate_test_cases(project_id):
    """åŸºäºä»£ç å˜æ›´ç”Ÿæˆæ™ºèƒ½æµ‹è¯•ç”¨ä¾‹"""
    try:
        data = request.get_json()
        changes = data.get('changes', [])
        use_qwen = data.get('use_qwen', False)
        project_context = data.get('project_context', {})
        
        if not changes:
            return jsonify({'error': 'æ²¡æœ‰æä¾›ä»£ç å˜æ›´ä¿¡æ¯'}), 400
        
        # è·å–é¡¹ç›®ä¿¡æ¯
        db = get_db()
        project = db.fetch_one("SELECT * FROM projects WHERE id = %s", (project_id,))
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        # å°è¯•ä½¿ç”¨ç´¢å¼•å¢å¼ºçš„Qwenç”Ÿæˆå™¨
        if use_qwen:
            try:
                # è·å–é¡¹ç›®ä»£ç åº“è·¯å¾„ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                codebase_path = None
                if project.get('local_path'):
                    codebase_path = project['local_path']
                elif project.get('git_url'):
                    # ä¸´æ—¶å…‹éš†ä»¥å»ºç«‹ç´¢å¼•
                    from ..utils.git_utils import GitUtils
                    git_utils = GitUtils(project['git_url'], project.get('branch', 'main'))
                    if git_utils.clone_repo():
                        codebase_path = git_utils.temp_dir
                
                # åˆ›å»ºç´¢å¼•å¢å¼ºçš„Qwenç”Ÿæˆå™¨
                qwen_generator = QwenTestGenerator(
                    api_key=os.getenv('QWEN_API_KEY'),
                    codebase_path=codebase_path
                )
                
                # ç”Ÿæˆæ™ºèƒ½æµ‹è¯•ç”¨ä¾‹
                test_cases = qwen_generator.analyze_code_changes_with_context(changes, project_context)
                
                # è½¬æ¢ä¸ºAPIå“åº”æ ¼å¼
                formatted_cases = []
                for case in test_cases:
                    formatted_case = {
                        'name': case.name,
                        'description': case.description,
                        'test_type': case.test_type,
                        'priority': case.priority,
                        'affected_components': case.affected_components,
                        'test_code': case.test_code,
                        'file_path': case.file_path,
                        'estimated_time': case.estimated_time,
                        'risk_level': case.risk_level,
                        # æ–°å¢ç´¢å¼•å¢å¼ºå­—æ®µ
                        'related_symbols': getattr(case, 'related_symbols', []),
                        'dependencies': getattr(case, 'dependencies', []),
                        'similar_tests': getattr(case, 'similar_tests', [])
                    }
                    formatted_cases.append(formatted_case)
                
                # æ¸…ç†ä¸´æ—¶ç›®å½•
                if project.get('git_url') and 'git_utils' in locals():
                    git_utils.cleanup()
                
                return jsonify({
                    'message': 'æ™ºèƒ½æµ‹è¯•ç”¨ä¾‹ç”ŸæˆæˆåŠŸ',
                    'test_cases': formatted_cases,
                    'generator_type': 'qwen_with_index',
                    'total_cases': len(formatted_cases),
                    'enhanced_features': {
                        'symbol_analysis': True,
                        'dependency_tracking': True,
                        'context_awareness': True,
                        'similarity_matching': True
                    }
                })
                
            except Exception as e:
                logger.error(f"Qwenæ™ºèƒ½ç”Ÿæˆå¤±è´¥: {str(e)}")
                # é™çº§åˆ°åŸºç¡€Qwenç”Ÿæˆå™¨
                try:
                    basic_qwen = QwenTestGenerator(api_key=os.getenv('QWEN_API_KEY'))
                    test_cases = basic_qwen.analyze_code_changes_with_context(changes, project_context)
                    
                    return jsonify({
                        'message': 'ä½¿ç”¨åŸºç¡€Qwenç”Ÿæˆå™¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹',
                        'test_cases': [asdict(case) for case in test_cases],
                        'generator_type': 'qwen_basic',
                        'warning': 'ç´¢å¼•åŠŸèƒ½ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€ç”Ÿæˆå™¨'
                    })
                except Exception as qwen_error:
                    logger.error(f"åŸºç¡€Qwenç”Ÿæˆä¹Ÿå¤±è´¥: {str(qwen_error)}")
                    # ç»§ç»­æ‰§è¡Œå›é€€é€»è¾‘
        
        # å›é€€åˆ°æœ¬åœ°ç”Ÿæˆ
        logger.info("ä½¿ç”¨æœ¬åœ°æ™ºèƒ½ç”Ÿæˆå™¨")
        test_cases = generate_smart_default_test_cases(changes, project_context)
        
        return jsonify({
            'message': 'ä½¿ç”¨æœ¬åœ°ç”Ÿæˆå™¨ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹',
            'test_cases': test_cases,
            'generator_type': 'local_enhanced',
            'total_cases': len(test_cases)
        })
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

def generate_smart_default_test_cases(changes: List[Dict], project_context: Dict = None) -> List[Dict]:
    """ç”Ÿæˆæ™ºèƒ½é»˜è®¤æµ‹è¯•ç”¨ä¾‹ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰"""
    test_cases = []
    
    for i, change in enumerate(changes):
        file_path = change.get('file', change.get('file_path', ''))
        change_type = change.get('type', 'modified')
        content = change.get('content', '')
        
        if not file_path:
            continue
        
        # åŸºç¡€åˆ†æ
        code_analyzer = CodeAnalyzer()
        analysis = {}
        if content:
            analysis = code_analyzer.analyze_file_content(file_path, content)
        
        # ç¡®å®šæµ‹è¯•ç‰¹å¾
        test_type, priority = determine_test_characteristics_enhanced(file_path, change_type, analysis)
        
        # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        test_case = {
            'name': f"æ™ºèƒ½æœ¬åœ°æµ‹è¯• - {os.path.basename(file_path)} ({change_type})",
            'description': generate_smart_description(file_path, analysis, change_type),
            'test_type': test_type,
            'priority': priority,
            'affected_components': identify_smart_components(file_path, analysis),
            'test_code': generate_smart_test_code(file_path, test_type, analysis),
            'file_path': file_path,
            'estimated_time': estimate_smart_time(test_type, analysis),
            'risk_level': assess_smart_risk(file_path, change_type, analysis),
            'code_analysis': analysis  # åŒ…å«åˆ†æç»“æœ
        }
        
        test_cases.append(test_case)
    
    return test_cases

def determine_test_characteristics_enhanced(file_path: str, change_type: str, analysis: Dict) -> tuple:
    """åŸºäºåˆ†æç»“æœç¡®å®šæµ‹è¯•ç‰¹å¾"""
    # åŸºç¡€åˆ¤æ–­
    if 'api' in file_path.lower() or 'route' in file_path.lower():
        base_type, base_priority = 'integration', 'high'
    elif file_path.endswith('.vue') or file_path.endswith('.js'):
        base_type, base_priority = 'e2e', 'medium'
    else:
        base_type, base_priority = 'unit', 'medium'
    
    # åŸºäºåˆ†æç»“æœè°ƒæ•´
    functions = analysis.get('functions', [])
    classes = analysis.get('classes', [])
    api_endpoints = analysis.get('api_endpoints', [])
    
    # å¤æ‚åº¦è°ƒæ•´
    if len(functions) > 5 or len(classes) > 2:
        if base_type == 'unit':
            base_type = 'integration'
        base_priority = 'high'
    
    # APIç«¯ç‚¹è°ƒæ•´
    if api_endpoints:
        base_type = 'integration'
        base_priority = 'high'
    
    return base_type, base_priority

def generate_smart_description(file_path: str, analysis: Dict, change_type: str) -> str:
    """ç”Ÿæˆæ™ºèƒ½æè¿°"""
    parts = [f"éªŒè¯ {os.path.basename(file_path)} åœ¨{change_type}åçš„åŠŸèƒ½"]
    
    functions = analysis.get('functions', [])
    if functions:
        func_names = [f['name'] for f in functions[:3]]
        parts.append(f"åŒ…å«å‡½æ•°: {', '.join(func_names)}")
    
    classes = analysis.get('classes', [])
    if classes:
        class_names = [c['name'] for c in classes[:2]]
        parts.append(f"åŒ…å«ç±»: {', '.join(class_names)}")
    
    api_endpoints = analysis.get('api_endpoints', [])
    if api_endpoints:
        parts.append(f"æ¶‰åŠ {len(api_endpoints)} ä¸ªAPIç«¯ç‚¹")
    
    return "ï¼Œ".join(parts)

def identify_smart_components(file_path: str, analysis: Dict) -> List[str]:
    """æ™ºèƒ½è¯†åˆ«å—å½±å“ç»„ä»¶"""
    components = set()
    
    # åŸºç¡€è¯†åˆ«
    if 'api' in file_path.lower():
        components.update(['APIæ¥å£', 'ä¸šåŠ¡é€»è¾‘'])
    if 'model' in file_path.lower():
        components.add('æ•°æ®æ¨¡å‹')
    if file_path.endswith('.vue'):
        components.update(['Vueç»„ä»¶', 'ç”¨æˆ·ç•Œé¢'])
    
    # åŸºäºåˆ†æç»“æœ
    if analysis.get('database_operations'):
        components.add('æ•°æ®åº“æ“ä½œ')
    if analysis.get('external_calls'):
        components.add('å¤–éƒ¨æœåŠ¡')
    if analysis.get('api_endpoints'):
        components.add('APIæœåŠ¡')
    
    return list(components) if components else ['æ ¸å¿ƒåŠŸèƒ½']

def generate_smart_test_code(file_path: str, test_type: str, analysis: Dict) -> str:
    """åŸºäºåˆ†æç”Ÿæˆæ™ºèƒ½æµ‹è¯•ä»£ç """
    if test_type == 'unit':
        return generate_smart_unit_test(file_path, analysis)
    elif test_type == 'integration':
        return generate_smart_integration_test(file_path, analysis)
    else:
        return generate_smart_e2e_test(file_path, analysis)

def generate_smart_unit_test(file_path: str, analysis: Dict) -> str:
    """ç”Ÿæˆæ™ºèƒ½å•å…ƒæµ‹è¯•"""
    functions = analysis.get('functions', [])
    
    if file_path.endswith('.py'):
        if functions:
            main_func = functions[0]
            return f"""
import unittest
from unittest.mock import Mock, patch

class Test{main_func['name'].title()}(unittest.TestCase):
    \"\"\"æ™ºèƒ½ç”Ÿæˆçš„å•å…ƒæµ‹è¯•\"\"\"
    
    def test_{main_func['name']}_basic(self):
        \"\"\"æµ‹è¯•åŸºæœ¬åŠŸèƒ½\"\"\"
        # å‚æ•°: {main_func.get('args', [])}
        # å¤æ‚åº¦: {main_func.get('complexity', 1)}
        pass
        
    def test_{main_func['name']}_edge_cases(self):
        \"\"\"æµ‹è¯•è¾¹ç•Œæ¡ä»¶\"\"\"
        pass
"""
    else:  # JavaScript
        if functions:
            main_func = functions[0]
            return f"""
import {{ describe, test, expect }} from '@jest/globals';

describe('{main_func['name']} æ™ºèƒ½æµ‹è¯•', () => {{
    test('åŸºæœ¬åŠŸèƒ½éªŒè¯', () => {{
        // å¤æ‚åº¦: {main_func.get('complexity', 1)}
        expect({main_func['name']}).toBeDefined();
    }});
    
    test('å‚æ•°éªŒè¯', () => {{
        // å‚æ•°: {main_func.get('args', [])}
        expect(() => {main_func['name']}()).not.toThrow();
    }});
}});
"""
    
    return "// æ™ºèƒ½å•å…ƒæµ‹è¯•æ¨¡æ¿"

def generate_smart_integration_test(file_path: str, analysis: Dict) -> str:
    """ç”Ÿæˆæ™ºèƒ½é›†æˆæµ‹è¯•"""
    api_endpoints = analysis.get('api_endpoints', [])
    
    if api_endpoints and file_path.endswith('.py'):
        return f"""
import unittest
import requests

class IntegrationTest(unittest.TestCase):
    \"\"\"æ™ºèƒ½é›†æˆæµ‹è¯•\"\"\"
    
    def setUp(self):
        self.base_url = 'http://localhost:5000'
    
    def test_api_endpoints(self):
        \"\"\"æµ‹è¯•APIç«¯ç‚¹\"\"\"
        # æ£€æµ‹åˆ°çš„ç«¯ç‚¹: {api_endpoints}
        for endpoint in {api_endpoints}:
            response = requests.get(f'{{self.base_url}}{{endpoint}}')
            self.assertEqual(response.status_code, 200)
"""
    
    return "// æ™ºèƒ½é›†æˆæµ‹è¯•æ¨¡æ¿"

def generate_smart_e2e_test(file_path: str, analysis: Dict) -> str:
    """ç”Ÿæˆæ™ºèƒ½E2Eæµ‹è¯•"""
    return f"""
import {{ test, expect }} from '@playwright/test';

test('E2EåŠŸèƒ½æµ‹è¯•', async ({{ page }}) => {{
    await page.goto('http://localhost:3000');
    
    // åŸºäºæ–‡ä»¶: {os.path.basename(file_path)}
    // åŠŸèƒ½æ•°é‡: {len(analysis.get('functions', []))}
    
    await expect(page.locator('body')).toBeVisible();
}});
"""

def estimate_smart_time(test_type: str, analysis: Dict) -> int:
    """æ™ºèƒ½ä¼°ç®—æ—¶é—´"""
    base_times = {'unit': 8, 'integration': 15, 'e2e': 25}
    base_time = base_times.get(test_type, 10)
    
    # åŸºäºå¤æ‚åº¦è°ƒæ•´
    function_count = len(analysis.get('functions', []))
    if function_count > 3:
        base_time += function_count * 2
    
    return base_time

def assess_smart_risk(file_path: str, change_type: str, analysis: Dict) -> str:
    """æ™ºèƒ½é£é™©è¯„ä¼°"""
    if change_type == 'deleted':
        return 'high'
    
    # åŸºäºAPIæ•°é‡
    api_count = len(analysis.get('api_endpoints', []))
    if api_count > 2:
        return 'high'
    elif api_count > 0:
        return 'medium'
    
    return 'low' if change_type == 'added' else 'medium'

# ç”Ÿæˆå™¨ç›¸å…³å¯¼å…¥
try:
    from generators.qwen_test_generator import QwenTestGenerator, CodeAnalyzer
except ImportError:
    logger.warning("Qwen test generator not available")
    QwenTestGenerator = None
    
    # åˆ›å»ºç®€åŒ–çš„CodeAnalyzerç±»ä½œä¸ºå›é€€
    class CodeAnalyzer:
        def analyze_file_content(self, file_path: str, content: str) -> Dict:
            return {
                'functions': [],
                'classes': [],
                'api_endpoints': [],
                'database_operations': [],
                'external_calls': []
            }

# ========== åŸºäºç´¢å¼•çš„é«˜çº§åˆ†æç«¯ç‚¹ ==========

@api.route('/projects/<int:project_id>/index-analysis', methods=['POST'])
def analyze_with_index(project_id):
    """åŸºäºä»£ç ç´¢å¼•çš„é«˜çº§å·®å¼‚åˆ†æ"""
    try:
        data = request.get_json()
        commit_hash = data.get('commit_hash')
        base_commit = data.get('base_commit')
        force_rebuild_index = data.get('force_rebuild_index', False)
        
        # è·å–é¡¹ç›®ä¿¡æ¯
        project = db.fetch_one("SELECT * FROM projects WHERE id = %s", (project_id,))
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        # è·å–é¡¹ç›®è·¯å¾„ - è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
        project_path = project.get('path', '')
        temp_cleanup_needed = False
        
        if not project_path:
            # å¦‚æœé¡¹ç›®æ²¡æœ‰æœ¬åœ°è·¯å¾„ï¼Œå°è¯•ä»git_urlæ¨æ–­æˆ–åˆ›å»ºä¸´æ—¶è·¯å¾„
            git_url = project.get('git_url', '')
            if git_url:
                # ä½¿ç”¨GitUtilså…‹éš†é¡¹ç›®åˆ°ä¸´æ—¶ç›®å½•
                try:
                    git_utils = GitUtils(git_url, branch=project.get('branch', 'main'))
                    if git_utils.clone_repo():
                        project_path = git_utils.temp_dir  # ä½¿ç”¨GitUtilsåˆ›å»ºçš„ä¸´æ—¶ç›®å½•
                        temp_cleanup_needed = True
                    else:
                        return jsonify({'error': 'Failed to clone repository'}), 500
                except Exception as e:
                    logger.error(f"å…‹éš†ä»“åº“å¤±è´¥: {str(e)}")
                    return jsonify({'error': f'Failed to clone repository: {str(e)}'}), 500
            else:
                return jsonify({'error': 'Project path or git URL not found'}), 404
        
        if not os.path.exists(project_path):
            return jsonify({'error': 'Project path not found'}), 404
        
        # åˆå§‹åŒ–Gitå·¥å…·
        git_utils = None
        repo_url = project.get('git_url')
        if repo_url:
            try:
                git_utils = GitUtils(repo_url, branch=project.get('branch', 'main'))
                git_utils.temp_dir = project_path
                # å¦‚æœæ˜¯ä¸´æ—¶ç›®å½•ï¼Œrepoå·²ç»åœ¨å…‹éš†æ—¶åˆå§‹åŒ–
                if not hasattr(git_utils, 'repo') or not git_utils.repo:
                    # åˆå§‹åŒ–æœ¬åœ°ä»“åº“
                    import git
                    git_utils.repo = git.Repo(project_path)
            except Exception as e:
                logger.warning(f"Gitåˆå§‹åŒ–å¤±è´¥: {str(e)}")
                git_utils = None
        
        # åˆ›å»ºåŸºäºç´¢å¼•çš„åˆ†æå™¨
        try:
            from analyzers.index_based_analyzer import IndexBasedAnalyzer
            analyzer = IndexBasedAnalyzer(project_path, git_utils)
        except ImportError as e:
            logger.error(f"å¯¼å…¥IndexBasedAnalyzerå¤±è´¥: {str(e)}")
            return jsonify({'error': 'Index-based analyzer not available'}), 500
        
        # æ‰§è¡Œç»¼åˆåˆ†æ
        logger.info(f"å¼€å§‹åŸºäºç´¢å¼•çš„ç»¼åˆåˆ†æ: project_id={project_id}, commit={commit_hash}")
        analysis_result = analyzer.analyze_comprehensive_diff(commit_hash, base_commit)
        
        # ä¿å­˜åˆ†æç»“æœåˆ°æ•°æ®åº“
        try:
            # åˆ›å»ºcode_analysisè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            db.execute('''
                CREATE TABLE IF NOT EXISTS code_analysis (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    project_id INTEGER NOT NULL,
                    commit_hash TEXT,
                    base_commit TEXT,
                    analysis_type TEXT NOT NULL,
                    analysis_data TEXT NOT NULL,
                    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (project_id) REFERENCES projects (id)
                )
            ''')
            
            # æ’å…¥åˆ†æè®°å½•
            analysis_id = db.execute(
                '''INSERT INTO code_analysis 
                   (project_id, commit_hash, base_commit, analysis_type, analysis_data) 
                   VALUES (%s, %s, %s, %s, %s)''',
                (
                    project_id,
                    commit_hash or 'HEAD',
                    base_commit or 'parent',
                    'index_based',
                    json.dumps(analysis_result)
                )
            )
            
            # æ·»åŠ åˆ†æIDåˆ°ç»“æœä¸­
            analysis_result['analysis_id'] = analysis_id
            
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {str(e)}")
            # ç»§ç»­è¿”å›ç»“æœï¼Œå³ä½¿ä¿å­˜å¤±è´¥
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•ï¼ˆå¦‚æœä½¿ç”¨äº†ï¼‰
        if temp_cleanup_needed and git_utils and hasattr(git_utils, 'cleanup'):
            try:
                git_utils.cleanup()
            except:
                pass
        
        return jsonify({
            'success': True,
            'analysis': analysis_result
        })
        
    except Exception as e:
        logger.error(f"åŸºäºç´¢å¼•çš„åˆ†æå¤±è´¥: {str(e)}")
        return jsonify({
            'error': str(e),
            'details': 'åŸºäºç´¢å¼•çš„åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯'
        }), 500

@api.route('/projects/<int:project_id>/build-index', methods=['POST'])
def build_project_index(project_id):
    """æ„å»ºé¡¹ç›®ä»£ç ç´¢å¼•"""
    try:
        data = request.get_json() or {}
        force_rebuild = data.get('force_rebuild', False)
        
        # è·å–é¡¹ç›®ä¿¡æ¯
        project = db.fetch_one("SELECT * FROM projects WHERE id = %s", (project_id,))
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        # è·å–æˆ–åˆ›å»ºé¡¹ç›®è·¯å¾„
        project_path = project.get('path', '')
        temp_cleanup_needed = False
        
        if not project_path:
            # åˆ›å»ºä¸´æ—¶è·¯å¾„å¹¶å…‹éš†é¡¹ç›®
            git_url = project.get('git_url', '')
            if git_url:
                try:
                    git_utils = GitUtils(git_url, branch=project.get('branch', 'main'))
                    if git_utils.clone_repo():
                        project_path = git_utils.temp_dir  # ä½¿ç”¨GitUtilsåˆ›å»ºçš„ä¸´æ—¶ç›®å½•
                        temp_cleanup_needed = True
                    else:
                        return jsonify({'error': 'Failed to clone repository'}), 500
                except Exception as e:
                    logger.error(f"å…‹éš†ä»“åº“å¤±è´¥: {str(e)}")
                    return jsonify({'error': f'Failed to clone repository: {str(e)}'}), 500
            else:
                return jsonify({'error': 'Project path or git URL not found'}), 404
        
        if not os.path.exists(project_path):
            return jsonify({'error': 'Project path not found'}), 404
        
        # åˆ›å»ºç´¢å¼•å™¨
        try:
            from indexers.codebase_indexer import CodebaseIndexer
            indexer = CodebaseIndexer(index_dir=os.path.join(project_path, '.code_index'))
        except ImportError as e:
            logger.error(f"å¯¼å…¥CodebaseIndexerå¤±è´¥: {str(e)}")
            return jsonify({'error': 'Codebase indexer not available'}), 500
        
        # æ„å»ºç´¢å¼•
        logger.info(f"å¼€å§‹æ„å»ºé¡¹ç›®ç´¢å¼•: project_id={project_id}, force_rebuild={force_rebuild}")
        
        try:
            if force_rebuild or not indexer.load_index():
                index_stats = indexer.build_index(project_path)
            else:
                # åŠ è½½ç°æœ‰ç´¢å¼•çš„ç»Ÿè®¡ä¿¡æ¯
                index_stats = {
                    "symbol_count": len(indexer.symbol_index),
                    "module_count": len(indexer.module_index),
                    "index_path": indexer.index_dir,
                    "loaded_from_cache": True
                }
        except Exception as e:
            logger.error(f"æ„å»ºç´¢å¼•å¤±è´¥: {str(e)}")
            return jsonify({'error': f'Failed to build index: {str(e)}'}), 500
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            if temp_cleanup_needed:
                try:
                    import shutil
                    shutil.rmtree(project_path, ignore_errors=True)
                except:
                    pass
        
        return jsonify({
            'success': True,
            'index_stats': index_stats
        })
        
    except Exception as e:
        logger.error(f"æ„å»ºç´¢å¼•å¤±è´¥: {str(e)}")
        return jsonify({
            'error': str(e),
            'details': 'æ„å»ºé¡¹ç›®ç´¢å¼•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯'
        }), 500

@api.route('/projects/<int:project_id>/index-status', methods=['GET'])
def get_index_status(project_id):
    """è·å–é¡¹ç›®ç´¢å¼•çŠ¶æ€"""
    try:
        # è·å–é¡¹ç›®ä¿¡æ¯
        project = db.fetch_one("SELECT * FROM projects WHERE id = %s", (project_id,))
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        project_path = project.get('path', '')
        if not project_path:
            # å¦‚æœæ²¡æœ‰æœ¬åœ°è·¯å¾„ï¼Œç´¢å¼•ä¸å­˜åœ¨
            return jsonify({
                'success': True,
                'index_status': {
                    'exists': False,
                    'symbol_count': 0,
                    'module_count': 0,
                    'index_path': None,
                    'last_updated': None,
                    'requires_clone': True
                }
            })
        
        if not os.path.exists(project_path):
            return jsonify({
                'success': True,
                'index_status': {
                    'exists': False,
                    'symbol_count': 0,
                    'module_count': 0,
                    'index_path': None,
                    'last_updated': None,
                    'path_not_found': True
                }
            })
        
        # æ£€æŸ¥ç´¢å¼•çŠ¶æ€
        try:
            from indexers.codebase_indexer import CodebaseIndexer
            indexer = CodebaseIndexer(index_dir=os.path.join(project_path, '.code_index'))
        except ImportError:
            return jsonify({'error': 'Codebase indexer not available'}), 500
        
        index_exists = indexer.load_index()
        
        if index_exists:
            status = {
                'exists': True,
                'symbol_count': len(indexer.symbol_index),
                'module_count': len(indexer.module_index),
                'index_path': indexer.index_dir,
                'last_updated': None
            }
            
            # è·å–ç´¢å¼•æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
            try:
                index_file = os.path.join(indexer.index_dir, "symbol_index.json")
                if os.path.exists(index_file):
                    mtime = os.path.getmtime(index_file)
                    status['last_updated'] = mtime
            except:
                pass
        else:
            status = {
                'exists': False,
                'symbol_count': 0,
                'module_count': 0,
                'index_path': indexer.index_dir,
                'last_updated': None
            }
        
        return jsonify({
            'success': True,
            'index_status': status
        })
        
    except Exception as e:
        logger.error(f"è·å–ç´¢å¼•çŠ¶æ€å¤±è´¥: {str(e)}")
        return jsonify({
            'error': str(e),
            'details': 'è·å–ç´¢å¼•çŠ¶æ€è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯'
        }), 500

@api.route('/projects/<int:project_id>/analysis-history', methods=['GET'])
def get_analysis_history(project_id):
    """è·å–é¡¹ç›®çš„åˆ†æå†å²è®°å½•"""
    try:
        analysis_type = request.args.get('type', 'all')  # 'all', 'git_diff', 'index_based'
        limit = int(request.args.get('limit', 20))
        
        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        where_clause = "WHERE project_id = %s"
        params = [project_id]
        
        if analysis_type != 'all':
            where_clause += " AND analysis_type = %s"
            params.append(analysis_type)
        
        # ç¡®ä¿code_analysisè¡¨å­˜åœ¨
        db.execute('''
            CREATE TABLE IF NOT EXISTS code_analysis (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                project_id INTEGER NOT NULL,
                commit_hash TEXT,
                base_commit TEXT,
                analysis_type TEXT NOT NULL,
                analysis_data TEXT NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY (project_id) REFERENCES projects (id)
            )
        ''')
        
        # æŸ¥è¯¢åˆ†æå†å²
        query = f'''
            SELECT id, commit_hash, base_commit, analysis_type, created_at, analysis_data
            FROM code_analysis 
            {where_clause}
            ORDER BY created_at DESC 
            LIMIT %s
        '''
        
        records = db.fetch_all(query, params + [limit])
        
        history = []
        for record in records:
            # è§£æsummaryä¿¡æ¯
            summary = {}
            try:
                analysis_data = json.loads(record[5]) if record[5] else {}
                summary = analysis_data.get('summary', {})
            except:
                pass
            
            history.append({
                'id': record[0],
                'commit_hash': record[1],
                'base_commit': record[2],
                'analysis_type': record[3],
                'created_at': record[4],
                'summary': summary
            })
        
        return jsonify({
            'success': True,
            'history': history,
            'total': len(history)
        })
        
    except Exception as e:
        logger.error(f"è·å–åˆ†æå†å²å¤±è´¥: {str(e)}")
        return jsonify({
            'error': str(e),
            'details': 'è·å–åˆ†æå†å²è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯'
        }), 500

@api.route('/projects/<int:project_id>/analysis/<int:analysis_id>', methods=['GET'])
def get_analysis_detail(project_id, analysis_id):
    """è·å–ç‰¹å®šåˆ†æçš„è¯¦ç»†ç»“æœ"""
    try:
        record = db.fetch_one('''
            SELECT analysis_data, analysis_type, commit_hash, base_commit, created_at
            FROM code_analysis 
            WHERE id = %s AND project_id = %s
        ''', (analysis_id, project_id))
        
        if not record:
            return jsonify({'error': 'Analysis not found'}), 404
        
        analysis_data = json.loads(record[0]) if record[0] else {}
        
        return jsonify({
            'success': True,
            'analysis': {
                'id': analysis_id,
                'data': analysis_data,
                'type': record[1],
                'commit_hash': record[2],
                'base_commit': record[3],
                'created_at': record[4]
            }
        })
        
    except Exception as e:
        logger.error(f"è·å–åˆ†æè¯¦æƒ…å¤±è´¥: {str(e)}")
        return jsonify({
            'error': str(e),
            'details': 'è·å–åˆ†æè¯¦æƒ…è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯'
        }), 500

@api.route('/projects/<int:id>/intelligent-test-cases', methods=['GET'])
def get_intelligent_test_cases(id):
    """è·å–æ™ºèƒ½ç”Ÿæˆçš„åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹"""
    try:
        logger.info(f"ğŸ§  ç”Ÿæˆæ™ºèƒ½åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹ for project {id}")
        
        # è·å–é¡¹ç›®ä¿¡æ¯
        db = get_db()
        project = db.fetch_one("SELECT * FROM projects WHERE id = ?", (id,))
        if not project:
            return jsonify({'error': 'Project not found'}), 404
        
        project_path = project.get('git_url', '')
        project_name = project.get('name', f'Project {id}')
        
        # å°è¯•ä½¿ç”¨å¢å¼ºçš„AIå®¢æˆ·ç«¯ç”ŸæˆåŠŸèƒ½æµ‹è¯•ç”¨ä¾‹
        try:
            from clients.enhanced_ai_client import EnhancedAIClient
            
            # åˆ›å»ºAIå®¢æˆ·ç«¯
            ai_client = EnhancedAIClient()
            
            # ç”ŸæˆåŠŸèƒ½æµ‹è¯•ç”¨ä¾‹ - ä¿®å¤å¼‚æ­¥è°ƒç”¨é—®é¢˜
            import asyncio
            try:
                # åˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯è€Œä¸æ˜¯è·å–ç°æœ‰çš„
                try:
                    loop = asyncio.get_event_loop()
                    if loop.is_running():
                        # å¦‚æœå½“å‰çº¿ç¨‹å·²æœ‰è¿è¡Œçš„å¾ªç¯ï¼Œä½¿ç”¨asyncio.run
                        import concurrent.futures
                        with concurrent.futures.ThreadPoolExecutor() as executor:
                            future = executor.submit(
                                asyncio.run,
                                ai_client.generate_comprehensive_functional_tests(
                                    change_analysis={
                                        'project_path': project_path,
                                        'project_name': project_name,
                                        'project_id': id
                                    },
                                    system_context={
                                        'project_type': 'web_application',
                                        'tech_stack': ['python', 'javascript'],
                                        'user_roles': ['admin', 'user']
                                    }
                                )
                            )
                            intelligent_tests = future.result(timeout=30)
                    else:
                        intelligent_tests = loop.run_until_complete(
                            ai_client.generate_comprehensive_functional_tests(
                                change_analysis={
                                    'project_path': project_path,
                                    'project_name': project_name,
                                    'project_id': id
                                },
                                system_context={
                                    'project_type': 'web_application',
                                    'tech_stack': ['python', 'javascript'],
                                    'user_roles': ['admin', 'user']
                                }
                            )
                        )
                except RuntimeError:
                    # æ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
                    intelligent_tests = asyncio.run(
                        ai_client.generate_comprehensive_functional_tests(
                            change_analysis={
                                'project_path': project_path,
                                'project_name': project_name,
                                'project_id': id
                            },
                            system_context={
                                'project_type': 'web_application',
                                'tech_stack': ['python', 'javascript'],
                                'user_roles': ['admin', 'user']
                            }
                        )
                    )
            except Exception as async_error:
                logger.warning(f"å¼‚æ­¥è°ƒç”¨å¤±è´¥: {async_error}")
                # å›é€€åˆ°åŒæ­¥æ–¹å¼ç”Ÿæˆé»˜è®¤ç”¨ä¾‹
                intelligent_tests = None
            
            # å¦‚æœAIç”ŸæˆæˆåŠŸ
            if intelligent_tests and isinstance(intelligent_tests, (list, dict)):
                # å¤„ç†AIè¿”å›çš„æ•°æ®æ ¼å¼
                processed_tests = None
                
                if isinstance(intelligent_tests, dict):
                    # æ£€æŸ¥æ˜¯å¦æœ‰raw_responseå­—æ®µï¼ˆAIæ¨¡å‹è¿”å›çš„åŸå§‹JSONï¼‰
                    if 'raw_response' in intelligent_tests:
                        try:
                            # è§£æraw_responseä¸­çš„JSON
                            raw_response = intelligent_tests['raw_response']
                            if raw_response.startswith('```json'):
                                # ç§»é™¤markdownä»£ç å—æ ‡è®°
                                json_start = raw_response.find('{')
                                json_end = raw_response.rfind('}') + 1
                                if json_start != -1 and json_end > json_start:
                                    json_str = raw_response[json_start:json_end]
                                    parsed_data = json.loads(json_str)
                                    
                                    # æå–test_planæ•°ç»„
                                    if 'test_plan' in parsed_data:
                                        processed_tests = parsed_data['test_plan']
                                        logger.info(f"âœ… è§£æAIç”Ÿæˆçš„æµ‹è¯•è®¡åˆ’ï¼ŒåŒ…å« {len(processed_tests)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
                                    else:
                                        processed_tests = parsed_data
                            else:
                                # ç›´æ¥è§£æJSON
                                parsed_data = json.loads(raw_response)
                                processed_tests = parsed_data.get('test_plan', parsed_data)
                        except (json.JSONDecodeError, KeyError) as parse_error:
                            logger.warning(f"è§£æAIå“åº”å¤±è´¥: {parse_error}")
                            processed_tests = None
                    else:
                        # ç›´æ¥ä½¿ç”¨è¿”å›çš„æ•°æ®
                        processed_tests = intelligent_tests
                elif isinstance(intelligent_tests, list):
                    processed_tests = intelligent_tests
                
                # å¦‚æœæˆåŠŸè§£æå‡ºæµ‹è¯•ç”¨ä¾‹
                if processed_tests and len(processed_tests) > 0:
                    logger.info(f"âœ… æˆåŠŸç”Ÿæˆ {len(processed_tests)} ä¸ªæ™ºèƒ½åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹")
                    return jsonify(processed_tests)
                else:
                    logger.warning("AIç”Ÿæˆçš„æ•°æ®æ ¼å¼ä¸æ­£ç¡®æˆ–ä¸ºç©º")
            else:
                logger.warning("AIç”Ÿæˆå¤±è´¥æˆ–è¿”å›ç©ºç»“æœ")
            
        except Exception as e:
            logger.warning(f"AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¤±è´¥: {e}")
        
        # å¦‚æœAIç”Ÿæˆå¤±è´¥ï¼Œè¿”å›é»˜è®¤æ™ºèƒ½æµ‹è¯•ç”¨ä¾‹
        logger.info("ä½¿ç”¨é»˜è®¤æ™ºèƒ½æµ‹è¯•ç”¨ä¾‹")
        test_cases = generate_default_intelligent_tests(project_name, id)
        return jsonify(test_cases)
            
    except Exception as e:
        logger.error(f"è·å–æ™ºèƒ½æµ‹è¯•ç”¨ä¾‹å¤±è´¥: {str(e)}")
        return jsonify({'error': str(e)}), 500

def generate_default_intelligent_tests(project_name, project_id):
    """ç”Ÿæˆé»˜è®¤çš„æ™ºèƒ½åŠŸèƒ½æµ‹è¯•ç”¨ä¾‹"""
    return [
        {
            'test_case_name': f'{project_name} - æ ¸å¿ƒåŠŸèƒ½éªŒè¯',
            'test_type': 'functional',
            'business_scenario': f'éªŒè¯{project_name}é¡¹ç›®çš„æ ¸å¿ƒåŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ',
            'test_steps': [
                '1. å¯åŠ¨åº”ç”¨ç¨‹åºå¹¶éªŒè¯åˆå§‹åŒ–',
                '2. éªŒè¯ä¸»è¦åŠŸèƒ½æ¨¡å—æ­£å¸¸åŠ è½½',
                '3. æ‰§è¡Œæ ¸å¿ƒä¸šåŠ¡æµç¨‹æ“ä½œ',
                '4. æ£€æŸ¥è¾“å‡ºç»“æœçš„æ­£ç¡®æ€§',
                '5. éªŒè¯å¼‚å¸¸æƒ…å†µçš„å¤„ç†æœºåˆ¶'
            ],
            'expected_result': 'æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½åº”æ­£å¸¸æ‰§è¡Œï¼Œå¼‚å¸¸æƒ…å†µåº”æœ‰åˆé€‚çš„é”™è¯¯å¤„ç†',
            'priority': 'high',
            'estimated_time': 30,
            'preconditions': 'åº”ç”¨ç¨‹åºç¯å¢ƒå·²é…ç½®å®Œæˆï¼Œæµ‹è¯•æ•°æ®å·²å‡†å¤‡',
            'test_data': 'æ ‡å‡†æµ‹è¯•æ•°æ®é›†ï¼ŒåŒ…å«æ­£å¸¸å’Œè¾¹ç•Œå€¼',
            'generation_method': 'default_intelligent'
        },
        {
            'test_case_name': f'{project_name} - æ•°æ®å¤„ç†å®Œæ•´æ€§éªŒè¯',
            'test_type': 'functional',
            'business_scenario': f'éªŒè¯{project_name}é¡¹ç›®çš„æ•°æ®è¾“å…¥ã€å¤„ç†å’Œè¾“å‡ºå®Œæ•´æ€§',
            'test_steps': [
                '1. å‡†å¤‡å¤šç§ç±»å‹çš„æµ‹è¯•æ•°æ®',
                '2. è¾“å…¥æ•°æ®åˆ°ç³»ç»Ÿå¹¶éªŒè¯æ¥æ”¶',
                '3. æ‰§è¡Œæ•°æ®å¤„ç†é€»è¾‘å¹¶ç›‘æ§è¿‡ç¨‹',
                '4. éªŒè¯å¤„ç†ç»“æœçš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§',
                '5. æ£€æŸ¥æ•°æ®è¾“å‡ºæ ¼å¼æ˜¯å¦ç¬¦åˆè§„èŒƒ'
            ],
            'expected_result': 'æ•°æ®å¤„ç†åº”ä¿æŒå‡†ç¡®æ€§å’Œä¸€è‡´æ€§ï¼Œè¾“å‡ºæ ¼å¼åº”ç¬¦åˆé¢„æœŸè§„èŒƒ',
            'priority': 'high',
            'estimated_time': 25,
            'preconditions': 'æµ‹è¯•æ•°æ®å·²å‡†å¤‡å®Œæˆï¼Œæ•°æ®å¤„ç†æ¨¡å—æ­£å¸¸',
            'test_data': 'åŒ…å«è¾¹ç•Œå€¼ã€å¼‚å¸¸å€¼å’Œå¤§å®¹é‡æ•°æ®çš„æµ‹è¯•é›†',
            'generation_method': 'default_intelligent'
        },
        {
            'test_case_name': f'{project_name} - ç”¨æˆ·äº¤äº’ä½“éªŒéªŒè¯',
            'test_type': 'functional',
            'business_scenario': f'éªŒè¯{project_name}é¡¹ç›®çš„ç”¨æˆ·ç•Œé¢å’Œäº¤äº’åŠŸèƒ½å¯ç”¨æ€§',
            'test_steps': [
                '1. æ‰“å¼€ç”¨æˆ·ç•Œé¢å¹¶æ£€æŸ¥åŠ è½½çŠ¶æ€',
                '2. æµ‹è¯•å„ä¸ªäº¤äº’å…ƒç´ çš„å“åº”æ€§',
                '3. éªŒè¯ç”¨æˆ·æ“ä½œçš„å®æ—¶åé¦ˆ',
                '4. æ£€æŸ¥é”™è¯¯æç¤ºçš„å‡†ç¡®æ€§å’Œå‹å¥½æ€§',
                '5. æµ‹è¯•ä¸åŒåœºæ™¯ä¸‹çš„ç•Œé¢è¡¨ç°'
            ],
            'expected_result': 'ç•Œé¢å…ƒç´ åº”æ­£ç¡®æ˜¾ç¤ºå’Œå“åº”ï¼Œç”¨æˆ·æ“ä½œåº”å¾—åˆ°åŠæ—¶å‡†ç¡®çš„åé¦ˆ',
            'priority': 'medium',
            'estimated_time': 20,
            'preconditions': 'ç”¨æˆ·ç•Œé¢å·²æ­£å¸¸åŠ è½½ï¼Œæµ‹è¯•ç¯å¢ƒç¨³å®š',
            'test_data': 'ä¸åŒç”¨æˆ·è§’è‰²çš„æµ‹è¯•è´¦å·å’Œæ“ä½œåœºæ™¯',
            'generation_method': 'default_intelligent'
        }
    ]